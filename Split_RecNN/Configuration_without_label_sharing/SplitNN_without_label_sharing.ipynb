{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87963dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "np.random.seed(666)\n",
    "from Distributed_HM_Data import Distributed_HM, binary_acc\n",
    "\n",
    "dataDir = Path.cwd().parent/'Data/'\n",
    "\n",
    "# model will train on CPU since PySyft 0.2.9 exist bugs with CUDA\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3348013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "# preserve training log\n",
    "so = open(\"config2.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe49009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMSaleTrainDataLoader(Dataset):\n",
    "    \"\"\"HMSaleTrainDataLoader Training set of HM sales data\n",
    "\n",
    "    Args:\n",
    "        transactions (pd.DataFrame): Dataframe of transaction records\n",
    "        all_products_id (list): A list contains all product ids\n",
    "    \"\"\"\n",
    "    def __init__(self, transactions, all_products_id):\n",
    "        self.customers, self.products, self.prices, self.sales_channels, \\\n",
    "        self.club_status, self.age_groups, self.product_groups, self.color_groups, \\\n",
    "        self.index_name, self.labels = self.get_dataset(transactions, all_products_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.customers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.customers[idx], self.products[idx], self.prices[idx], self.sales_channels[idx], self.club_status[idx], \\\n",
    "               self.age_groups[idx], self.product_groups[idx], self.color_groups[idx], self.index_name[idx], self.labels[idx]\n",
    "    \n",
    "    def get_dataset(self, transactions, all_products_id):\n",
    "        customers, products, prices, sales_channels, club_status, age_groups, product_groups, color_groups, index_name, labels  = [], [], [], [], [], [], [], [], [], []\n",
    "        customer_product_set = set(zip(transactions[\"customer_id\"], transactions[\"article_id\"], \n",
    "                                       transactions[\"price\"], transactions[\"sales_channel_id\"], \n",
    "                                       transactions[\"club_member_status\"], transactions[\"age\"], \n",
    "                                       transactions[\"product_group_name\"], transactions[\"colour_group_name\"], transactions[\"index_name\"]))\n",
    "        \n",
    "        \"\"\"negative sampling\"\"\"\n",
    "        # set up negative:positive ratio as 4:1\n",
    "        negative_samples = 4\n",
    "\n",
    "        for u, i, price, sale, club, age, product, color, index in tqdm(customer_product_set):\n",
    "            customers.append(u)\n",
    "            products.append(i)\n",
    "            prices.append(price)\n",
    "            sales_channels.append(sale)\n",
    "            club_status.append(club)\n",
    "            age_groups.append(age)\n",
    "            product_groups.append(product)\n",
    "            color_groups.append(color)\n",
    "            index_name.append(index)\n",
    "            labels.append(1)\n",
    "            for _ in range(negative_samples):\n",
    "                negative_product = np.random.choice(all_products_id)\n",
    "                while (u, negative_product, price, sale, club, age, product, color, index) in customer_product_set:\n",
    "                    negative_product = np.random.choice(all_products_id)\n",
    "                customers.append(u)\n",
    "                products.append(negative_product)\n",
    "                prices.append(price)\n",
    "                sales_channels.append(sale)\n",
    "                club_status.append(club)\n",
    "                age_groups.append(age)\n",
    "                product_groups.append(product)\n",
    "                color_groups.append(color)\n",
    "                index_name.append(index)\n",
    "                labels.append(0)\n",
    "        \n",
    "        customers = torch.tensor(customers)\n",
    "        products = torch.tensor(products)\n",
    "        prices = torch.tensor(prices)\n",
    "        sales_channels = torch.tensor(sales_channels)\n",
    "        club_status = torch.tensor(club_status)\n",
    "        age_groups = torch.tensor(age_groups)\n",
    "        product_groups = torch.tensor(product_groups)\n",
    "        color_groups = torch.tensor(color_groups)\n",
    "        index_name = torch.tensor(index_name)\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        return customers, products, prices, sales_channels, club_status, age_groups, product_groups, color_groups, index_name, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddec9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesNN(nn.Module):\n",
    "    \"\"\" Partial model for sales domain\n",
    "    Args:\n",
    "        num_users (int): Number of users\n",
    "        num_items (int): Number of products\n",
    "        prices (float): price of transactions\n",
    "        sales_channels (float): sales channels\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_users: int, \n",
    "            num_items: int,\n",
    "            input_size: int = 50,\n",
    "            user_embedding_dim: int = 16,\n",
    "            item_embedding_dim: int = 32,\n",
    "            hidden_size_1: int = 128,\n",
    "            output_size: int = 32,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.user_embedding_layer = nn.Embedding(num_embeddings=num_users, embedding_dim=user_embedding_dim)\n",
    "        self.item_embedding_layer = nn.Embedding(num_embeddings=num_items, embedding_dim=item_embedding_dim)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        if num_users and num_items is not None:\n",
    "                in_channels = (\n",
    "                    [input_size] \n",
    "                    + [hidden_size_1]\n",
    "                    + [output_size]\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_input, item_input, prices, sales_channels):\n",
    "        user_embedding = self.user_embedding_layer(user_input)\n",
    "        item_embedding = self.item_embedding_layer(item_input)\n",
    "#         user_embedding = torch.squeeze(user_embedding, dim=1)\n",
    "#         item_embedding = torch.squeeze(item_embedding, dim=1)\n",
    "        \n",
    "        latent_vec = torch.cat([user_embedding, item_embedding, prices, sales_channels], dim=-1)\n",
    "        for layer in self.encoder:\n",
    "            latent_vec = layer(latent_vec)\n",
    "            latent_vec = self.relu(latent_vec)\n",
    "        \n",
    "        return latent_vec\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "    \n",
    "class CustomersNN(nn.Module):\n",
    "    \"\"\" Partial model for customer domain\n",
    "    Args:\n",
    "        club_status (int): active or inactive customers' status\n",
    "        age_groups (int): age of customers\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 2,\n",
    "            output_size: int = 5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [output_size]\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, club_status, age_groups):\n",
    "        \n",
    "        latent_vec = torch.cat([club_status, age_groups], dim=-1)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            latent_vec = layer(latent_vec)\n",
    "            latent_vec = self.relu(latent_vec)\n",
    "        \n",
    "        return latent_vec\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class ProductsNN(nn.Module):\n",
    "    \"\"\" Partial model for product domain\n",
    "    Args:\n",
    "        num_product_groups (int): Number of product groups\n",
    "        num_color_groups: (int): Number of color groups\n",
    "        num_index_name: (int): Number of index name\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_product_groups: int,\n",
    "            num_color_groups: int,\n",
    "            num_index_name: int,\n",
    "            product_group_embedding_dim: int = 8,\n",
    "            color_group_embedding_dim: int = 16,\n",
    "            index_name_embedding_dim: int = 6,\n",
    "            input_size: int = 30,\n",
    "            hidden_size_1: int = 64,\n",
    "            output_size: int = 16,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.product_group_embedding_layer = nn.Embedding(num_embeddings=num_product_groups, embedding_dim=product_group_embedding_dim)\n",
    "        self.color_group_embedding_layer = nn.Embedding(num_embeddings=num_color_groups, embedding_dim=color_group_embedding_dim)\n",
    "        self.index_name_embedding_layer = nn.Embedding(num_embeddings=num_index_name, embedding_dim=index_name_embedding_dim)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        if num_product_groups and num_color_groups and num_index_name is not None:\n",
    "                in_channels = (\n",
    "                    [input_size] \n",
    "                    + [hidden_size_1]\n",
    "                    + [output_size]\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, product_groups, color_groups, index_name):\n",
    "        product_group_embedding = self.product_group_embedding_layer(product_groups)\n",
    "        color_group_embedding = self.color_group_embedding_layer(color_groups)\n",
    "        index_name_embedding = self.index_name_embedding_layer(index_name)\n",
    "#         product_group_embedding = torch.squeeze(product_group_embedding, dim=1)\n",
    "#         color_group_embedding = torch.squeeze(color_group_embedding, dim=1)\n",
    "#         index_name_embedding = torch.squeeze(index_name_embedding, dim=1)\n",
    "        \n",
    "        latent_vec = torch.cat([product_group_embedding, color_group_embedding, index_name_embedding], dim=-1)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            latent_vec = layer(latent_vec)\n",
    "            latent_vec = self.relu(latent_vec)\n",
    "        \n",
    "        return latent_vec\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class GovernanceNN(nn.Module):\n",
    "    \"\"\" Partial model for goverance side\n",
    "    Args:\n",
    "        agg_latent_input (int): aggregated input of latent vectors from client models\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 53,\n",
    "            hidden_size_1: int = 128,\n",
    "            hidden_size_2: int = 64,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [hidden_size_1]\n",
    "            + [hidden_size_2]\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "    def forward(self, agg_latent_input):\n",
    "        \n",
    "        for layer in self.decoder:\n",
    "            agg_latent_input = layer(agg_latent_input)\n",
    "            agg_latent_input = self.relu(agg_latent_input)\n",
    "        \n",
    "        out = agg_latent_input\n",
    "        \n",
    "        return out\n",
    "\n",
    "class LabelOwner(nn.Module):\n",
    "    \"\"\" Partial model for label owner\n",
    "    Args:\n",
    "        agg_latent_input (int): aggregated input of latent vectors from client models\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 64,\n",
    "            output_size: int = 2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, server_out):\n",
    "        \n",
    "        out = self.lin(server_out)\n",
    "        return out   \n",
    "\n",
    "\n",
    "class SplitNN(nn.Module):\n",
    "    def __init__(self, models, optimizers, data_owner, server, label_owner):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        self.data_owners = data_owner\n",
    "        self.server = server\n",
    "        self.label_owner = label_owner\n",
    "        \n",
    "#         self.outputs = [None]*len(self.models)\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, data_pointer):\n",
    "        \n",
    "        #individual client's output upto their respective cut layer\n",
    "        client_output = {}\n",
    "\n",
    "        #outputs that is moved to server and subjected to concatenate for server input\n",
    "        remote_output = []\n",
    "        \n",
    "        for owner in data_owners:\n",
    "            if owner.id == \"sales_domain\":\n",
    "                client_output[owner.id] = models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1], data_pointer[owner.id][2], data_pointer[owner.id][3])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"customer_domain\":\n",
    "                client_output[owner.id] = models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"product_domain\":\n",
    "                client_output[owner.id] = models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1], data_pointer[owner.id][2])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(server, requires_grad=True)\n",
    "                )\n",
    "        # concat outputs from clients and send to server side\n",
    "        server_input = torch.cat(remote_output, dim=-1)\n",
    "        # make prediction on server model and send to the label owner client\n",
    "        server_output = models[\"server\"](server_input)\n",
    "        server_output.move(label_owner, requires_grad=True)\n",
    "        pred = models[\"label_owner\"](server_output)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "    \n",
    "    def train(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].train()\n",
    "#             if loc == \"server\":\n",
    "#                 for i in range(len(self.models[loc])):\n",
    "#                     self.models[loc][i].train()\n",
    "#             else:\n",
    "#                 self.models[loc].train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].eval()        \n",
    "    \n",
    "    def load_weights(self, file_prefix):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].load_state_dict(torch.load(f\"{file_prefix}_{loc}_weights.pth\"))\n",
    "            \n",
    "    @property\n",
    "    def location(self):\n",
    "        return self.models[0].location if self.models and len(self.models) else None\n",
    "    \n",
    "def train(x, label, splitNN):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "  \n",
    "    #3) Figure out how much we missed by\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(pred, label)\n",
    "    \n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #5) Feed Gradients backward through the network\n",
    "    #splitNN.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss.detach().get()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08bafba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0060156e324813a90270d80b716d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 880\n",
      "num_items: 16450\n",
      "num_product_groups: 13\n",
      "num_color_groups: 49\n"
     ]
    }
   ],
   "source": [
    "hm_data = pd.read_csv(dataDir/'medium_train.csv')\n",
    "all_products_id = hm_data[\"article_id\"].unique()\n",
    "train_data = HMSaleTrainDataLoader(hm_data, all_products_id)\n",
    "train_loader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "# set up virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "sales_domain = sy.VirtualWorker(hook, id=\"sales_domain\")\n",
    "customer_domain = sy.VirtualWorker(hook, id=\"customer_domain\")\n",
    "product_domain = sy.VirtualWorker(hook, id=\"product_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "label_owner = sy.VirtualWorker(hook, id=\"label_owner\")\n",
    "\n",
    "data_owners = (sales_domain, customer_domain, product_domain)\n",
    "model_locations = [sales_domain, customer_domain, product_domain, server, label_owner]\n",
    "\n",
    "distributed_trainloader = Distributed_HM(data_owners=data_owners, data_loader=train_loader)\n",
    "\n",
    "# set up parameters for model\n",
    "num_users = len(hm_data.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "num_product_groups = len(hm_data.product_group_name.unique())\n",
    "print(\"num_product_groups:\", num_product_groups)\n",
    "num_color_groups = len(hm_data.colour_group_name.unique())\n",
    "print(\"num_color_groups:\", num_color_groups)\n",
    "num_index_name = len(hm_data.index_name.unique())\n",
    "\n",
    "models = {\n",
    "    \"sales_domain\": SalesNN(num_users=num_users, num_items=num_items),\n",
    "    \"customer_domain\": CustomersNN(),\n",
    "    \"product_domain\": ProductsNN(num_product_groups=num_product_groups, num_color_groups=num_color_groups, num_index_name=num_index_name),\n",
    "    \"server\": GovernanceNN(),\n",
    "    \"label_owner\": LabelOwner(),\n",
    "}\n",
    "\n",
    "# set up optimizer for clients' model\n",
    "optimizers = [\n",
    "    optim.Adam(models[location.id].parameters(), lr=0.003)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4fb6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sales_domain': SalesNN(\n",
      "  (user_embedding_layer): Embedding(880, 16)\n",
      "  (item_embedding_layer): Embedding(16450, 32)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "), 'customer_domain': CustomersNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "  )\n",
      "), 'product_domain': ProductsNN(\n",
      "  (product_group_embedding_layer): Embedding(13, 8)\n",
      "  (color_group_embedding_layer): Embedding(49, 16)\n",
      "  (index_name_embedding_layer): Embedding(10, 6)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=16, bias=True)\n",
      "  )\n",
      "), 'server': GovernanceNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=53, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "), 'label_owner': LabelOwner(\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")}\n",
      "Epoch 0 - Training loss: 0.5073716044425964\n",
      "Epoch 1 - Training loss: 0.4997885525226593\n",
      "Epoch 2 - Training loss: 0.49305349588394165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1814/104727168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributed_trainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_owner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1814/3503202853.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, label, splitNN)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m#6) Change the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0msplitNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1814/3503202853.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/pointers.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             message = TensorCommandMessage.computation(\n\u001b[0;32m--> 523\u001b[0;31m                 \u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m             )\n\u001b[1;32m    525\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/messaging/message.py\u001b[0m in \u001b[0;36mcomputation\u001b[0;34m(name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdirectly\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/execution/computation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/execution/action.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(models)\n",
    "\n",
    "epochs = 150\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "splitnn = SplitNN(models, optimizers, data_owners, server, label_owner)\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    splitnn.train()\n",
    "    for data_ptr, labels in distributed_trainloader:  \n",
    "        labels = labels.send(label_owner)\n",
    "        loss = train(data_ptr, labels, splitnn)\n",
    "        running_loss += loss\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(distributed_trainloader)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5db1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
