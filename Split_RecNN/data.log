num_users: 880
num_items: 16450
num_product_groups: 13
num_color_groups: 49
{'sales_domain': SalesNN(
  (user_embedding_layer): Embedding(880, 16)
  (item_embedding_layer): Embedding(16450, 32)
  (relu): LeakyReLU(negative_slope=0.01)
), 'customer_domain': CustomersNN(
  (relu): LeakyReLU(negative_slope=0.01)
  (encoder): Sequential(
    (0): Linear(in_features=2, out_features=5, bias=True)
  )
), 'product_domain': ProductsNN(
  (product_group_embedding_layer): Embedding(13, 8)
  (color_group_embedding_layer): Embedding(49, 16)
  (index_name_embedding_layer): Embedding(10, 6)
  (relu): LeakyReLU(negative_slope=0.01)
), 'server': GovernanceNN(
  (relu): LeakyReLU(negative_slope=0.01)
  (decoder): Sequential(
    (0): Linear(in_features=85, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)}
Epoch 0 - Training loss: 0.5060088038444519
Epoch 1 - Training loss: 0.4972097873687744
Epoch 2 - Training loss: 0.4861961603164673
Epoch 3 - Training loss: 0.4612663984298706
Epoch 4 - Training loss: 0.4228629171848297
Epoch 5 - Training loss: 0.3746678829193115
Epoch 6 - Training loss: 0.326033353805542
Epoch 7 - Training loss: 0.28514277935028076
Epoch 8 - Training loss: 0.24810917675495148
Epoch 9 - Training loss: 0.2235531359910965
Epoch 10 - Training loss: 0.2035963237285614
Epoch 11 - Training loss: 0.18503330647945404
Epoch 12 - Training loss: 0.16450707614421844
Epoch 13 - Training loss: 0.1461223065853119
Epoch 14 - Training loss: 0.13726820051670074
Epoch 15 - Training loss: 0.12711164355278015
Epoch 16 - Training loss: 0.12647046148777008
Epoch 17 - Training loss: 0.1337224692106247
Epoch 18 - Training loss: 0.13491764664649963
Epoch 19 - Training loss: 0.12170901149511337
Epoch 20 - Training loss: 0.10244170576334
Epoch 21 - Training loss: 0.08646190166473389
Epoch 22 - Training loss: 0.07711298763751984
Epoch 23 - Training loss: 0.07185237854719162
Epoch 24 - Training loss: 0.06544823944568634
Epoch 25 - Training loss: 0.05764974653720856
Epoch 26 - Training loss: 0.051245830953121185
Epoch 27 - Training loss: 0.045276105403900146
Epoch 28 - Training loss: 0.039917562156915665
Epoch 29 - Training loss: 0.03224470466375351
Epoch 30 - Training loss: 0.026352625340223312
Epoch 31 - Training loss: 0.021131254732608795
Epoch 32 - Training loss: 0.01731615886092186
Epoch 33 - Training loss: 0.013803213834762573
Epoch 34 - Training loss: 0.010923029854893684
Epoch 35 - Training loss: 0.008975891396403313
Epoch 36 - Training loss: 0.0077539728954434395
Epoch 37 - Training loss: 0.005984026938676834
Epoch 38 - Training loss: 0.004847200121730566
Epoch 39 - Training loss: 0.003961245995014906
Epoch 40 - Training loss: 0.0033968810457736254
Epoch 41 - Training loss: 0.0031027363147586584
Epoch 42 - Training loss: 0.002548139775171876
Epoch 43 - Training loss: 0.0022992498707026243
Epoch 44 - Training loss: 0.002241385169327259
Epoch 45 - Training loss: 0.002272906946018338
Epoch 46 - Training loss: 0.009942169301211834
Epoch 47 - Training loss: 0.019894512370228767
Epoch 48 - Training loss: 0.013099206611514091
Epoch 49 - Training loss: 0.00826350413262844
Epoch 50 - Training loss: 0.005120851099491119
Epoch 51 - Training loss: 0.0030340563971549273
Epoch 52 - Training loss: 0.0020228635985404253
Epoch 53 - Training loss: 0.0014454892370849848
Epoch 54 - Training loss: 0.0011875162599608302
Epoch 55 - Training loss: 0.0007652905769646168
Epoch 56 - Training loss: 0.000697675219271332
Epoch 57 - Training loss: 0.0006753027555532753
Epoch 58 - Training loss: 0.0005924651050008833
Epoch 59 - Training loss: 0.0007040503551252186
Epoch 60 - Training loss: 0.0005811784067191184
Epoch 61 - Training loss: 0.00054820446530357
Epoch 62 - Training loss: 0.0005857375799678266
Epoch 63 - Training loss: 0.0005157867562957108
Epoch 64 - Training loss: 0.00048374675679951906
Epoch 65 - Training loss: 0.0004966065171174705
Epoch 66 - Training loss: 0.00046562112402170897
Epoch 67 - Training loss: 0.0005236614961177111
Epoch 68 - Training loss: 0.0029487176798284054
Epoch 69 - Training loss: 0.025010740384459496
Epoch 70 - Training loss: 0.018046004697680473
Epoch 71 - Training loss: 0.008556284941732883
Epoch 72 - Training loss: 0.005191126838326454
Epoch 73 - Training loss: 0.0026418555062264204
Epoch 74 - Training loss: 0.0012429960770532489
Epoch 75 - Training loss: 0.0008197608985938132
Epoch 76 - Training loss: 0.0005187162314541638
Epoch 77 - Training loss: 0.0004465034871827811
Epoch 78 - Training loss: 0.0004068563866894692
Epoch 79 - Training loss: 0.0003926235076505691
Epoch 80 - Training loss: 0.0003835317911580205
Epoch 81 - Training loss: 0.0003790486662182957
Epoch 82 - Training loss: 0.00037973313010297716
Epoch 83 - Training loss: 0.0003707051801029593
Epoch 84 - Training loss: 0.0003916567948181182
Epoch 85 - Training loss: 0.0003874042595271021
Epoch 86 - Training loss: 0.000343073857948184
Epoch 87 - Training loss: 0.0006121756159700453
Epoch 88 - Training loss: 0.0009460806613788009
Epoch 89 - Training loss: 0.007416890934109688
Epoch 90 - Training loss: 0.014624858275055885
Epoch 91 - Training loss: 0.00975071731954813
Epoch 92 - Training loss: 0.005013397894799709
Epoch 93 - Training loss: 0.002565031638368964
Epoch 94 - Training loss: 0.0009675839683040977
Epoch 95 - Training loss: 0.000566971895750612
Epoch 96 - Training loss: 0.00046344712609425187
Epoch 97 - Training loss: 0.00033113412791863084
Epoch 98 - Training loss: 0.00027749952278099954
Epoch 99 - Training loss: 0.0002839700027834624
Epoch 100 - Training loss: 0.0003222832456231117
Epoch 101 - Training loss: 0.0003110704419668764
Epoch 102 - Training loss: 0.0003115520812571049
Epoch 103 - Training loss: 0.0003053029649890959
Epoch 104 - Training loss: 0.0002961723366752267
Epoch 105 - Training loss: 0.00028345041209831834
Epoch 106 - Training loss: 0.00028371188091114163
Epoch 107 - Training loss: 0.00031823155586607754
Epoch 108 - Training loss: 0.00034746163873933256
Epoch 109 - Training loss: 0.0009617533651180565
Epoch 110 - Training loss: 0.006620373111218214
Epoch 111 - Training loss: 0.01454208604991436
Epoch 112 - Training loss: 0.00774946715682745
Epoch 113 - Training loss: 0.0025622290559113026
Epoch 114 - Training loss: 0.0011262123007327318
Epoch 115 - Training loss: 0.0005546243628486991
Epoch 116 - Training loss: 0.0002694461727514863
Epoch 117 - Training loss: 0.0003166106471326202
Epoch 118 - Training loss: 0.00024051849322859198
Epoch 119 - Training loss: 0.0002398081123828888
Accuracy on dataset Test set is (88.3%)
OrderedDict([('user_embedding_layer.weight', tensor([[-1.7579, -1.7165,  0.2599,  ..., -0.5478,  1.2784, -0.0665],
        [ 0.0118,  0.3026,  0.5534,  ...,  0.5910, -0.2702,  0.2277],
        [ 0.0235, -1.2699, -1.0637,  ..., -0.1764, -0.3490,  0.5225],
        ...,
        [-0.1349, -0.6884,  1.2037,  ...,  0.6722,  1.1997, -0.0601],
        [ 0.3599,  0.0904, -2.1772,  ...,  0.5212,  1.3992,  0.2837],
        [ 0.6548, -1.1735, -0.1056,  ...,  0.0723, -1.3547,  1.0241]])), ('item_embedding_layer.weight', tensor([[-0.3329,  0.4583,  0.3010,  ...,  0.7664, -0.8303,  0.3522],
        [-1.2361,  0.1548,  2.2680,  ...,  0.4386,  0.4047, -0.6644],
        [ 0.2227,  1.8626, -2.2245,  ..., -0.3952, -0.4385, -0.6690],
        ...,
        [-0.5576,  0.6416, -0.2545,  ..., -1.4302,  1.1244, -0.1329],
        [-1.7545, -0.2886, -0.2520,  ..., -0.3235, -0.3648, -1.1586],
        [ 2.0036, -0.0974, -0.4583,  ..., -0.9719,  0.0235, -0.9348]]))])
odict_keys(['user_embedding_layer.weight', 'item_embedding_layer.weight'])
WARNING:root:Torch was already hooked... skipping hooking process
WARNING:root:Torch was already hooked... skipping hooking process
WARNING:root:Torch was already hooked... skipping hooking process
WARNING:root:Torch was already hooked... skipping hooking process
WARNING:root:Torch was already hooked... skipping hooking process
WARNING:root:Torch was already hooked... skipping hooking process
