{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabc0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "# np.random.seed(666)\n",
    "from Distributed_HM_Data import HMSaleTrainDataLoader, Distributed_HM, binary_acc\n",
    "from utils_models import SalesNN, CustomersNN, ProductsNN\n",
    "\n",
    "dataDir = Path.cwd().parent.parent/'Data/'\n",
    "\n",
    "# model will train on CPU since PySyft 0.2.9 exist bugs with CUDA\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3df47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "# preserve training log\n",
    "so = open(\"config1.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f92facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesNN(nn.Module):\n",
    "    \"\"\" Partial model for sales domain\n",
    "    Args:\n",
    "        num_users (int): Number of users\n",
    "        num_items (int): Number of products\n",
    "        prices (float): price of transactions\n",
    "        sales_channels (float): sales channels\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_users: int, \n",
    "            num_items: int,\n",
    "            user_embedding_dim: int = 32,\n",
    "            item_embedding_dim: int = 64,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.user_embedding_layer = nn.Embedding(num_embeddings=num_users, embedding_dim=user_embedding_dim)\n",
    "        self.item_embedding_layer = nn.Embedding(num_embeddings=num_items, embedding_dim=item_embedding_dim)\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_embedding_layer(user_input)\n",
    "        item_embedding = self.item_embedding_layer(item_input)\n",
    "\n",
    "        latent_vec = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "\n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "    \n",
    "class CustomersNN(nn.Module):\n",
    "    \"\"\" Partial model for customer domain\n",
    "    Args:\n",
    "        club_status (int): active or inactive customers' status\n",
    "        age_groups (int): age of customers\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 2,\n",
    "            output_size: int = 4,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [output_size]\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, club_status, age_groups):\n",
    "        \n",
    "        latent_vec = torch.cat([club_status, age_groups], dim=-1)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            latent_vec = layer(latent_vec)\n",
    "        \n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class ProductsNN(nn.Module):\n",
    "    \"\"\" Partial model for product domain\n",
    "    Args:\n",
    "        num_product_groups (int): Number of product groups\n",
    "        num_color_groups: (int): Number of color groups\n",
    "        num_index_name: (int): Number of index name\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_product_groups: int,\n",
    "            num_color_groups: int,\n",
    "            num_index_name: int,\n",
    "            product_group_embedding_dim: int = 5,\n",
    "            color_group_embedding_dim: int = 16,\n",
    "            index_name_embedding_dim: int = 3,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.product_group_embedding_layer = nn.Embedding(num_embeddings=num_product_groups, embedding_dim=product_group_embedding_dim)\n",
    "        self.color_group_embedding_layer = nn.Embedding(num_embeddings=num_color_groups, embedding_dim=color_group_embedding_dim)\n",
    "        self.index_name_embedding_layer = nn.Embedding(num_embeddings=num_index_name, embedding_dim=index_name_embedding_dim)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, product_groups, color_groups, index_name):\n",
    "        product_group_embedding = self.product_group_embedding_layer(product_groups)\n",
    "        color_group_embedding = self.color_group_embedding_layer(color_groups)\n",
    "        index_name_embedding = self.index_name_embedding_layer(index_name)\n",
    "\n",
    "        latent_vec = torch.cat([product_group_embedding, color_group_embedding, index_name_embedding], dim=-1)\n",
    "        \n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fc3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized models for specific configuration\n",
    "\n",
    "class GovernanceNN(nn.Module):\n",
    "    \"\"\" Partial model for goverance side\n",
    "    Args:\n",
    "        agg_latent_input (int): aggregated input of latent vectors from client models\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 124,\n",
    "            hidden_size_1: int = 256,\n",
    "            hidden_size_2: int = 128,\n",
    "            output_size: int = 1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [hidden_size_1]\n",
    "            + [hidden_size_2]\n",
    "#             + [hidden_size_2]\n",
    "            + [output_size]\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, agg_latent_input):\n",
    "        \n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            agg_latent_input = layer(agg_latent_input)\n",
    "            agg_latent_input = self.relu(agg_latent_input)\n",
    "        out = agg_latent_input\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "\n",
    "class SplitNN(nn.Module):\n",
    "    def __init__(self, models, optimizers, data_owner, server):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        self.data_owners = data_owner\n",
    "        self.server = server\n",
    "#         self.outputs = [None]*len(self.models)\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, data_pointer):\n",
    "        \n",
    "        #individual client's output upto their respective cut layer\n",
    "        client_output = {}\n",
    "\n",
    "        #outputs that is moved to server and subjected to concatenate for server input\n",
    "        remote_output = []\n",
    "        \n",
    "        for owner in self.data_owners:\n",
    "            if owner.id == \"sales_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"customer_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"product_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1], data_pointer[owner.id][2])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "        \n",
    "        # concat outputs from clients and send to server side\n",
    "        server_input = torch.cat(remote_output, dim=-1)\n",
    "        # make prediction on server model\n",
    "        pred = self.models[\"server\"](server_input)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "    \n",
    "    def train(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].train()\n",
    "#             if loc == \"server\":\n",
    "#                 for i in range(len(self.models[loc])):\n",
    "#                     self.models[loc][i].train()\n",
    "#             else:\n",
    "#                 self.models[loc].train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].eval()        \n",
    "            \n",
    "    @property\n",
    "    def location(self):\n",
    "        return self.models[0].location if self.models and len(self.models) else None\n",
    "    \n",
    "def train(x, target, splitNN):\n",
    "    # set up bias weight for negative sampling\n",
    "    weights = torch.tensor([1.0, 5.0])\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    # make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1])\n",
    "    loss = criterion(pred, target.float())\n",
    "    \n",
    "    # backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss.detach().get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521f60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740d2b578d8545a19cf3d67bb9b7e9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65552 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 3153\n",
      "num_items: 4807\n",
      "num_product_groups: 10\n",
      "num_color_groups: 48\n",
      "num_index_name: 7\n"
     ]
    }
   ],
   "source": [
    "hm_data = pd.read_csv(dataDir/'train_transactions_medium.csv')\n",
    "hm_test_data = pd.read_csv(dataDir/'test_transactions_medium.csv')\n",
    "article_data = pd.read_csv(dataDir/'article_data_medium.csv')\n",
    "all_products_id = article_data[\"article_id\"].unique()\n",
    "transactions = pd.concat([hm_data, hm_test_data], ignore_index=True)\n",
    "\n",
    "customer_product_set = set(zip(transactions[\"customer_id\"], transactions[\"article_id\"], \n",
    "                               transactions[\"club_member_status\"], transactions[\"age\"], \n",
    "                               transactions[\"product_group_name\"], transactions[\"colour_group_name\"], transactions[\"index_name\"]))\n",
    "\n",
    "train_data = HMSaleTrainDataLoader(hm_data, all_products_id, customer_product_set)\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "\n",
    "# set up virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "sales_domain = sy.VirtualWorker(hook, id=\"sales_domain\")\n",
    "customer_domain = sy.VirtualWorker(hook, id=\"customer_domain\")\n",
    "product_domain = sy.VirtualWorker(hook, id=\"product_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (sales_domain, customer_domain, product_domain)\n",
    "model_locations = [sales_domain, customer_domain, product_domain, server]\n",
    "\n",
    "distributed_trainloader = Distributed_HM(data_owners=data_owners, data_loader=train_loader)\n",
    "\n",
    "# set up parameters for model\n",
    "num_users = len(hm_data.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "num_product_groups = len(hm_data.product_group_name.unique())\n",
    "print(\"num_product_groups:\", num_product_groups)\n",
    "num_color_groups = len(hm_data.colour_group_name.unique())\n",
    "print(\"num_color_groups:\", num_color_groups)\n",
    "num_index_name = len(hm_data.index_name.unique())\n",
    "print(\"num_index_name:\", num_index_name)\n",
    "\n",
    "models = {\n",
    "    \"sales_domain\": SalesNN(num_users=num_users, num_items=num_items),\n",
    "    \"customer_domain\": CustomersNN(),\n",
    "    \"product_domain\": ProductsNN(num_product_groups=num_product_groups, num_color_groups=num_color_groups, num_index_name=num_index_name),\n",
    "    \"server\": GovernanceNN(),\n",
    "}\n",
    "\n",
    "# set up optimizer for clients' model\n",
    "\n",
    "optimizers = [\n",
    "    optim.Adam(models[location.id].parameters(), lr=0.005)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392a0423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sales_domain': SalesNN(\n",
      "  (user_embedding_layer): Embedding(3153, 32)\n",
      "  (item_embedding_layer): Embedding(4807, 64)\n",
      "), 'customer_domain': CustomersNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "  )\n",
      "), 'product_domain': ProductsNN(\n",
      "  (product_group_embedding_layer): Embedding(10, 5)\n",
      "  (color_group_embedding_layer): Embedding(48, 16)\n",
      "  (index_name_embedding_layer): Embedding(7, 3)\n",
      "), 'server': GovernanceNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")}\n",
      "Epoch 0 - Training loss: 1.04466712474823\n",
      "Epoch 1 - Training loss: 0.5728796720504761\n",
      "Epoch 2 - Training loss: 0.33706116676330566\n",
      "Epoch 3 - Training loss: 0.232522115111351\n",
      "Epoch 4 - Training loss: 0.17846405506134033\n",
      "Epoch 5 - Training loss: 0.1467956006526947\n",
      "Epoch 6 - Training loss: 0.12417841702699661\n",
      "Epoch 7 - Training loss: 0.1104859709739685\n",
      "Epoch 8 - Training loss: 0.10162702202796936\n",
      "Epoch 9 - Training loss: 0.09851697832345963\n",
      "Epoch 10 - Training loss: 0.08698423951864243\n",
      "Epoch 11 - Training loss: 0.08092835545539856\n",
      "Epoch 12 - Training loss: 0.07174326479434967\n",
      "Epoch 13 - Training loss: 0.06813875585794449\n",
      "Epoch 14 - Training loss: 0.05810071527957916\n",
      "Epoch 15 - Training loss: 0.05336571857333183\n",
      "Epoch 16 - Training loss: 0.05624906346201897\n",
      "Epoch 17 - Training loss: 0.04605778679251671\n",
      "Epoch 18 - Training loss: 0.04237857460975647\n",
      "Epoch 19 - Training loss: 0.040149759501218796\n",
      "Epoch 20 - Training loss: 0.03534211590886116\n",
      "Epoch 21 - Training loss: 0.03550766780972481\n",
      "Epoch 22 - Training loss: 0.03559873253107071\n",
      "Epoch 23 - Training loss: 0.029834896326065063\n",
      "Epoch 24 - Training loss: 0.03363223373889923\n",
      "Epoch 25 - Training loss: 0.03162812814116478\n",
      "Epoch 26 - Training loss: 0.027468366548419\n",
      "Epoch 27 - Training loss: 0.025206170976161957\n",
      "Epoch 28 - Training loss: 0.030489400029182434\n",
      "Epoch 29 - Training loss: 0.02630995213985443\n",
      "Epoch 30 - Training loss: 0.02718758024275303\n",
      "Epoch 31 - Training loss: 0.03009663335978985\n",
      "Epoch 32 - Training loss: 0.024296211078763008\n",
      "Epoch 33 - Training loss: 0.02199750579893589\n",
      "Epoch 34 - Training loss: 0.025940828025341034\n",
      "Epoch 35 - Training loss: 0.02228391170501709\n",
      "Epoch 36 - Training loss: 0.01672852225601673\n",
      "Epoch 37 - Training loss: 0.02446047030389309\n",
      "Epoch 38 - Training loss: 0.022506678476929665\n",
      "Epoch 39 - Training loss: 0.021118737757205963\n",
      "Epoch 40 - Training loss: 0.019242284819483757\n",
      "Epoch 41 - Training loss: 0.01954691857099533\n",
      "Epoch 42 - Training loss: 0.016073543578386307\n",
      "Epoch 43 - Training loss: 0.016408881172537804\n",
      "Epoch 44 - Training loss: 0.016980772837996483\n",
      "Epoch 45 - Training loss: 0.019924234598875046\n",
      "Epoch 46 - Training loss: 0.017780225723981857\n",
      "Epoch 47 - Training loss: 0.013221428729593754\n",
      "Epoch 48 - Training loss: 0.010355371981859207\n",
      "Epoch 49 - Training loss: 0.00883415061980486\n",
      "Epoch 50 - Training loss: 0.01779700629413128\n",
      "Epoch 51 - Training loss: 0.012891716323792934\n",
      "Epoch 52 - Training loss: 0.01475472841411829\n",
      "Epoch 53 - Training loss: 0.016694504767656326\n",
      "Epoch 54 - Training loss: 0.010378330014646053\n",
      "Epoch 55 - Training loss: 0.009236582554876804\n",
      "Epoch 56 - Training loss: 0.008274970576167107\n",
      "Epoch 57 - Training loss: 0.01185038685798645\n",
      "Epoch 58 - Training loss: 0.00868490245193243\n",
      "Epoch 59 - Training loss: 0.008167820051312447\n",
      "Epoch 60 - Training loss: 0.009676779620349407\n",
      "Epoch 61 - Training loss: 0.008138337172567844\n",
      "Epoch 62 - Training loss: 0.00897131022065878\n",
      "Epoch 63 - Training loss: 0.008992133662104607\n",
      "Epoch 64 - Training loss: 0.0073562501929700375\n",
      "Epoch 65 - Training loss: 0.009841158986091614\n",
      "Epoch 66 - Training loss: 0.005935829132795334\n",
      "Epoch 67 - Training loss: 0.013888918794691563\n",
      "Epoch 68 - Training loss: 0.010351909324526787\n",
      "Epoch 69 - Training loss: 0.003272981382906437\n",
      "Epoch 70 - Training loss: 0.008890478871762753\n",
      "Epoch 71 - Training loss: 0.006958611309528351\n",
      "Epoch 72 - Training loss: 0.0031938618049025536\n",
      "Epoch 73 - Training loss: 0.0038182490970939398\n",
      "Epoch 74 - Training loss: 0.0034523867070674896\n",
      "Epoch 75 - Training loss: 0.006803619209676981\n",
      "Epoch 76 - Training loss: 0.007861476391553879\n",
      "Epoch 77 - Training loss: 0.004742583259940147\n",
      "Epoch 78 - Training loss: 0.006241513881832361\n",
      "Epoch 79 - Training loss: 0.0024838061071932316\n",
      "Epoch 80 - Training loss: 0.002280227607116103\n",
      "Epoch 81 - Training loss: 0.008275897242128849\n",
      "Epoch 82 - Training loss: 0.007193384692072868\n",
      "Epoch 83 - Training loss: 0.005380900111049414\n",
      "Epoch 84 - Training loss: 0.0022677979432046413\n",
      "Epoch 85 - Training loss: 0.001417335239239037\n",
      "Epoch 86 - Training loss: 0.0005712461424991488\n",
      "Epoch 87 - Training loss: 0.007413939107209444\n",
      "Epoch 88 - Training loss: 0.009172425605356693\n",
      "Epoch 89 - Training loss: 0.0037223228719085455\n",
      "Epoch 90 - Training loss: 0.0024561318568885326\n",
      "Epoch 91 - Training loss: 0.0037700932007282972\n",
      "Epoch 92 - Training loss: 0.008290338329970837\n",
      "Epoch 93 - Training loss: 0.003519990248605609\n",
      "Epoch 94 - Training loss: 0.00335481739602983\n",
      "Epoch 95 - Training loss: 0.0067122322507202625\n",
      "Epoch 96 - Training loss: 0.0024411899503320456\n",
      "Epoch 97 - Training loss: 0.002763530006632209\n",
      "Epoch 98 - Training loss: 0.006561957765370607\n",
      "Epoch 99 - Training loss: 0.0052967979572713375\n",
      "Epoch 100 - Training loss: 0.005213055294007063\n",
      "Epoch 101 - Training loss: 0.0021305696573108435\n",
      "Epoch 102 - Training loss: 0.003269009292125702\n",
      "Epoch 103 - Training loss: 0.008444641716778278\n",
      "Epoch 104 - Training loss: 0.010251362808048725\n",
      "Epoch 105 - Training loss: 0.003752819960936904\n",
      "Epoch 106 - Training loss: 0.001015119138173759\n",
      "Epoch 107 - Training loss: 0.008083500899374485\n",
      "Epoch 108 - Training loss: 0.004801088944077492\n",
      "Epoch 109 - Training loss: 0.0011354942107573152\n",
      "Epoch 110 - Training loss: 0.00039511171053163707\n",
      "Epoch 111 - Training loss: 0.0002770968421828002\n",
      "Epoch 112 - Training loss: 0.008860708214342594\n",
      "Epoch 113 - Training loss: 0.009786602109670639\n",
      "Epoch 114 - Training loss: 0.003704533213749528\n",
      "Epoch 115 - Training loss: 0.0010407957015559077\n",
      "Epoch 116 - Training loss: 0.0006231743609532714\n",
      "Epoch 117 - Training loss: 0.005424668546766043\n",
      "Epoch 118 - Training loss: 0.00819814670830965\n",
      "Epoch 119 - Training loss: 0.008464825339615345\n",
      "Epoch 120 - Training loss: 0.0038083475083112717\n",
      "Epoch 121 - Training loss: 0.0015563666820526123\n",
      "Epoch 122 - Training loss: 0.003767063608393073\n",
      "Epoch 123 - Training loss: 0.0031776565592736006\n",
      "Epoch 124 - Training loss: 0.0008493903442285955\n",
      "Epoch 125 - Training loss: 0.002258030930534005\n",
      "Epoch 126 - Training loss: 0.0070540704764425755\n",
      "Epoch 127 - Training loss: 0.006542474031448364\n",
      "Epoch 128 - Training loss: 0.00575149105861783\n",
      "Epoch 129 - Training loss: 0.003207593457773328\n",
      "Epoch 130 - Training loss: 0.002093152841553092\n",
      "Epoch 131 - Training loss: 0.0018380549736320972\n",
      "Epoch 132 - Training loss: 0.0009833324002102017\n",
      "Epoch 133 - Training loss: 0.0004894984303973615\n",
      "Epoch 134 - Training loss: 7.62766576372087e-05\n",
      "Epoch 135 - Training loss: 0.007599459495395422\n",
      "Epoch 136 - Training loss: 0.0062570287846028805\n",
      "Epoch 137 - Training loss: 0.0014036190696060658\n",
      "Epoch 138 - Training loss: 0.006846947129815817\n",
      "Epoch 139 - Training loss: 0.00320319808088243\n",
      "Epoch 140 - Training loss: 0.0010186104336753488\n",
      "Epoch 141 - Training loss: 0.009423415176570415\n",
      "Epoch 142 - Training loss: 0.00537560461089015\n",
      "Epoch 143 - Training loss: 0.005955429747700691\n",
      "Epoch 144 - Training loss: 0.002712156856432557\n",
      "Epoch 145 - Training loss: 0.00111570383887738\n",
      "Epoch 146 - Training loss: 0.0003438130661379546\n",
      "Epoch 147 - Training loss: 0.0004848715616390109\n",
      "Epoch 148 - Training loss: 0.0036332495510578156\n",
      "Epoch 149 - Training loss: 0.013101216405630112\n",
      "Epoch 150 - Training loss: 0.003908598329871893\n",
      "Epoch 151 - Training loss: 0.0023246926721185446\n",
      "Epoch 152 - Training loss: 0.002851993776857853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 337, in handle_func_command\n",
      "    cmd, args_, kwargs_, return_args_type=True\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\", line 157, in unwrap_args_from_function\n",
      "    new_args = hook_args(args_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\", line 356, in <lambda>\n",
      "    return lambda x: f(lambdas, x)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\", line 551, in five_fold\n",
      "    lambdas[0](args_[0], **kwargs),\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\", line 331, in <lambda>\n",
      "    else lambda i: forward_func[type(i)](i)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook_args.py\", line 27, in <lambda>\n",
      "    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook_args.py\", line 27, in <genexpr>\n",
      "    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),\n",
      "syft.exceptions.PureFrameworkTensorFoundError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_156516/504512500.py\", line 12, in <module>\n",
      "    loss = train(data_ptr, labels, splitnn)\n",
      "  File \"/tmp/ipykernel_156516/4013794522.py\", line 115, in train\n",
      "    pred = splitNN.forward(x)\n",
      "  File \"/tmp/ipykernel_156516/4013794522.py\", line 72, in forward\n",
      "    client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1], data_pointer[owner.id][2])\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_156516/3475812656.py\", line 95, in forward\n",
      "    color_group_embedding = self.color_group_embedding_layer(color_groups)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/sparse.py\", line 114, in forward\n",
      "    self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\", line 335, in overloaded_func\n",
      "    response = handle_func_command(command)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 356, in handle_func_command\n",
      "    response = new_type.handle_func_command(new_command)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/pointers/object_pointer.py\", line 211, in handle_func_command\n",
      "    response = owner.send_command(location, cmd_name=cmd, args_=args_, kwargs_=kwargs_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\", line 525, in send_command\n",
      "    ret_val = self.send_msg(message, location=recipient)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\", line 316, in send_msg\n",
      "    bin_response = self._send_msg(bin_message, location)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\", line 12, in _send_msg\n",
      "    return location._recv_msg(message)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\", line 22, in _recv_msg\n",
      "    return self.recv_msg(message)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\", line 356, in recv_msg\n",
      "    response = handler.handle(msg)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/abstract/message_handler.py\", line 20, in handle\n",
      "    return self.routing_table[type(msg)](msg)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/message_handler.py\", line 55, in execute_tensor_command\n",
      "    return self.execute_computation_action(cmd.action)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/message_handler.py\", line 118, in execute_computation_action\n",
      "    response = command(*args_, **kwargs_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\", line 335, in overloaded_func\n",
      "    response = handle_func_command(command)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 380, in handle_func_command\n",
      "    response = cls._get_response(cmd, args_, kwargs_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 414, in _get_response\n",
      "    response = command_method(*args_, **kwargs_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/functional.py\", line 1484, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\", line 335, in overloaded_func\n",
      "    response = handle_func_command(command)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 380, in handle_func_command\n",
      "    response = cls._get_response(cmd, args_, kwargs_)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\", line 414, in _get_response\n",
      "    response = command_method(*args_, **kwargs_)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/haoyuan/.conda/envs/datamesh-dev/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    336\u001b[0m             new_args, new_kwargs, new_type, args_type = hook_args.unwrap_args_from_function(\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args_, kwargs_, return_args_type)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Try running it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mfive_fold\u001b[0;34m(lambdas, args_, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m     return (\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Last if not, rule is probably == 1 so use type to return the right transformation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mforward_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# And do this for all the args / rules provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m }\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m }\n",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_156516/504512500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_156516/4013794522.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, target, splitNN)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_156516/4013794522.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_pointer)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"product_domain\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mclient_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pointer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_pointer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_pointer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 remote_output.append(\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_156516/3475812656.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, product_groups, color_groups, index_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mproduct_group_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_group_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mcolor_group_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_group_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mindex_name_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/pointers/object_pointer.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/abstract/message_handler.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_tensor_command\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_computation_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_computation_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(cmd, args_, kwargs_)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(cmd, args_, kwargs_)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2080\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "print(models)\n",
    "\n",
    "epochs = 200\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "splitnn = SplitNN(models, optimizers, data_owners, server)\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    splitnn.train()\n",
    "    for data_ptr, labels in distributed_trainloader:  \n",
    "        labels = labels.send(server)\n",
    "        loss = train(data_ptr, labels, splitnn)\n",
    "        running_loss += loss\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(distributed_trainloader)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c31a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "         torch.save(models[loc].get().state_dict(), f\"{file_prefix}_{loc}_weights.pth\")\n",
    "    \n",
    "save_weights(models, \"Split_RecNN_medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b6ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "        model_weights = torch.load(f\"{file_prefix}_{loc}_weights.pth\")\n",
    "        models[loc].load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719cfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    acc = 0.0\n",
    "    \n",
    "    y_pred_label = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_pred = (y_pred_label == y_test).sum()\n",
    "    acc = correct_pred.item()/y_test.shape[0]\n",
    "    return acc\n",
    "\n",
    "def predict(splitnn, data_ptr, product, test_items):\n",
    "    hits_10 = 1\n",
    "    hits_5 = 1\n",
    "    \n",
    "    y_pred = splitnn.forward(data_ptr).get()\n",
    "#     y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred = y_pred.squeeze()\n",
    "    \n",
    "    top10_probs, top10_indices = torch.topk(torch.sigmoid(y_pred), 10)\n",
    "    # Convert the top 10 probabilities to a list\n",
    "    top10_items = [test_items[i].item() for i in top10_indices]\n",
    "    if product in top10_items:\n",
    "        hits_10 = 1\n",
    "    else:\n",
    "        hits_10 = 0\n",
    "    \n",
    "    top5_probs, topk_indices = torch.topk(torch.sigmoid(y_pred), 5)\n",
    "    # Convert the top 5 probabilities to a list\n",
    "    top5_items = [test_items[i].item() for i in topk_indices]\n",
    "    if product in top5_items:\n",
    "        hits_5 = 1\n",
    "    else:\n",
    "        hits_5 = 0\n",
    "    \n",
    "    return hits_5, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c049fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum((2**r - 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198f3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distributed_HM_test(Dataset):\n",
    "    def __init__(self, data_owners, customer_batch, product_batch, club_status_batch, age_groups_batch, product_groups_batch, color_groups_batch, index_name_batch, label_batch):\n",
    "        self.data_owners = data_owners\n",
    "        self.no_of_owner = len(data_owners)\n",
    "        self.customer_batch = customer_batch\n",
    "        self.product_batch = product_batch\n",
    "        self.club_status_batch = club_status_batch\n",
    "        self.age_groups_batch = age_groups_batch\n",
    "        self.product_groups_batch = product_groups_batch\n",
    "        self.color_groups_batch = color_groups_batch\n",
    "        self.index_name_batch = index_name_batch\n",
    "        self.label_batch = label_batch\n",
    "\n",
    "        self.data_pointer = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.curr_data_dict = {}\n",
    "        self.labels.append(label_batch.reshape(-1, 1))\n",
    "\n",
    "        # split data batch based on domains\n",
    "        sales_domain = [customer_batch, product_batch]\n",
    "        customer_domain = [club_status_batch.float(), age_groups_batch.float()]\n",
    "        product_domain = [product_groups_batch, color_groups_batch, index_name_batch]\n",
    "            \n",
    "        # set data owners for each domain team\n",
    "        sales_owner = self.data_owners[0]\n",
    "        customer_owner = self.data_owners[1]\n",
    "        product_owner = self.data_owners[2]\n",
    "            \n",
    "            \n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_domain:\n",
    "            sales_part_ptr.append(tensor.send(sales_owner))\n",
    "        self.curr_data_dict[sales_owner.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_domain:\n",
    "            customer_part_ptr.append(tensor.send(customer_owner))\n",
    "        self.curr_data_dict[customer_owner.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_domain:\n",
    "            product_part_ptr.append(tensor.send(product_owner))\n",
    "        self.curr_data_dict[product_owner.id] = product_part_ptr\n",
    "\n",
    "        self.data_pointer.append(self.curr_data_dict)\n",
    "            \n",
    "    def get_data(self):\n",
    "        return self.data_pointer[0], self.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54080f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 3153\n",
      "num_items: 4807\n",
      "num_product_groups: 10\n",
      "num_color_groups: 48\n"
     ]
    }
   ],
   "source": [
    "# model test\n",
    "train_transactions = pd.read_csv(dataDir/'train_transactions_medium.csv')\n",
    "test_transactions = pd.read_csv(dataDir/'test_transactions_medium.csv')\n",
    "article_data = pd.read_csv(dataDir/'article_data_medium.csv')\n",
    "customer_data = pd.read_csv(dataDir/'customer_data_medium.csv')\n",
    "\n",
    "all_products_id = article_data[\"article_id\"].unique()\n",
    "customer_product_test = set(zip(test_transactions[\"customer_id\"], test_transactions[\"article_id\"], \n",
    "                                test_transactions[\"club_member_status\"], test_transactions[\"age\"], \n",
    "                                test_transactions[\"product_group_name\"], test_transactions[\"colour_group_name\"], \n",
    "                                test_transactions[\"index_name\"]))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = train_transactions.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "# set up parameters for model\n",
    "num_users = len(hm_data.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "num_product_groups = len(hm_data.product_group_name.unique())\n",
    "print(\"num_product_groups:\", num_product_groups)\n",
    "num_color_groups = len(hm_data.colour_group_name.unique())\n",
    "print(\"num_color_groups:\", num_color_groups)\n",
    "num_index_name = len(hm_data.index_name.unique())\n",
    "\n",
    "# load the model parameters\n",
    "models = {\n",
    "    \"sales_domain\": SalesNN(num_users=num_users, num_items=num_items),\n",
    "    \"customer_domain\": CustomersNN(),\n",
    "    \"product_domain\": ProductsNN(num_product_groups=num_product_groups, num_color_groups=num_color_groups, num_index_name=num_index_name),\n",
    "    \"server\": GovernanceNN(),\n",
    "}\n",
    "\n",
    "load_weights(models, \"Split_RecNN_medium\")\n",
    "\n",
    "# set up virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "sales_domain = sy.VirtualWorker(hook, id=\"sales_domain\")\n",
    "customer_domain = sy.VirtualWorker(hook, id=\"customer_domain\")\n",
    "product_domain = sy.VirtualWorker(hook, id=\"product_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (sales_domain, customer_domain, product_domain)\n",
    "model_locations = [sales_domain, customer_domain, product_domain, server]\n",
    "\n",
    "# set up optimizer for clients' model\n",
    "optimizers = [\n",
    "    optim.Adam(models[location.id].parameters(), lr=0.005)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)\n",
    "\n",
    "# Load the weights locally\n",
    "splitnn = SplitNN(models, optimizers, data_owners, server)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6765986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dacae3775ce4132b999645c3b4e2019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Rate@5 is 0.436\n",
      "The Hit Rate@10 is 0.892\n",
      "The NDCG@10 is 0.405\n",
      "The Recall is 0.943\n"
     ]
    }
   ],
   "source": [
    "# model test\n",
    "# np.random.seed(28)\n",
    "splitnn.eval()\n",
    "hits = []\n",
    "pred_prob = []\n",
    "acc = []\n",
    "hits_10 = []\n",
    "hits_5 = []\n",
    "recall = []\n",
    "ndcgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for customer, product, club_status, age_groups, product_groups, color_groups, index_name in tqdm(customer_product_test):\n",
    "        \n",
    "        # select 29 products from item set that customer has no interactions\n",
    "        interacted_items = user_interacted_items[customer]\n",
    "        interacted_items = interacted_items + [product]\n",
    "        not_interacted_items = set(all_products_id) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 10, replace=False))\n",
    "        test_items = [product] + selected_not_interacted\n",
    "        \n",
    "        # get the other product features based on the selected test items\n",
    "        product_groups_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"product_group_name\"].to_numpy()).reshape(-1, 1)\n",
    "        color_groups_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"colour_group_name\"].to_numpy()).reshape(-1, 1)\n",
    "        index_name_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"index_name\"].to_numpy()).reshape(-1, 1)\n",
    "\n",
    "        test_items = torch.tensor(test_items).reshape(-1, 1)\n",
    "        customer_batch = torch.tensor([customer]*11).reshape(-1, 1)\n",
    "        club_status_batch = torch.tensor([club_status]*11).reshape(-1, 1)\n",
    "        age_groups_batch = torch.tensor([age_groups]*11).reshape(-1, 1)\n",
    "#         label_batch = torch.tensor([1]+[0]*10).reshape(-1, 1)\n",
    "        \n",
    "        # batch prediction on test items\n",
    "        batch_data_dict = {}\n",
    "        # split data batch based on domains\n",
    "        sales_batch = [customer_batch, test_items]\n",
    "        customer_batch = [club_status_batch.float(), age_groups_batch.float()]\n",
    "        product_batch = [product_groups_batch, color_groups_batch, index_name_batch]\n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_batch:\n",
    "            sales_part_ptr.append(tensor.send(sales_domain))\n",
    "        batch_data_dict[sales_domain.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_batch:\n",
    "            customer_part_ptr.append(tensor.send(customer_domain))\n",
    "        batch_data_dict[customer_domain.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_batch:\n",
    "            product_part_ptr.append(tensor.send(product_domain))\n",
    "        batch_data_dict[product_domain.id] = product_part_ptr\n",
    "        \n",
    "        y_pred = splitnn.forward(batch_data_dict).get()\n",
    "#         y_pred = torch.sigmoid(y_pred)\n",
    "        y_pred = y_pred.squeeze()\n",
    "\n",
    "        top10_probs, top10_indices = torch.topk(y_pred, 10)\n",
    "        # Convert the top 10 probabilities to a list\n",
    "        top10_items = [test_items[i].item() for i in top10_indices]\n",
    "        if product in top10_items:\n",
    "            hits_10.append(1)\n",
    "        else:\n",
    "            hits_10.append(0)\n",
    "\n",
    "        top5_probs, top5_indices = torch.topk(y_pred, 5)\n",
    "        # Convert the top 5 probabilities to a list\n",
    "        top5_items = [test_items[i].item() for i in top5_indices]\n",
    "        if product in top5_items:\n",
    "            hits_5.append(1)\n",
    "        else:\n",
    "            hits_5.append(0)\n",
    "    \n",
    "        \n",
    "        # single prediction on product\n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        single_data_ptr = {}\n",
    "        sales_data = [torch.tensor([customer]).reshape(-1, 1), torch.tensor([product]).reshape(-1, 1)]\n",
    "        customer_data = [torch.tensor([club_status]).reshape(-1, 1).float(), torch.tensor([age_groups]).reshape(-1, 1).float()]\n",
    "        product_data = [torch.tensor([product_groups]).reshape(-1, 1), torch.tensor([color_groups]).reshape(-1, 1), torch.tensor([index_name]).reshape(-1, 1)]\n",
    "        \n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_data:\n",
    "            sales_part_ptr.append(tensor.send(sales_domain))\n",
    "        single_data_ptr[sales_domain.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_data:\n",
    "            customer_part_ptr.append(tensor.send(customer_domain))\n",
    "        single_data_ptr[customer_domain.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_data:\n",
    "            product_part_ptr.append(tensor.send(product_domain))\n",
    "        single_data_ptr[product_domain.id] = product_part_ptr\n",
    "        \n",
    "        y_single_pred = splitnn.forward(single_data_ptr).get()\n",
    "        if torch.round(torch.sigmoid(y_single_pred)) == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)\n",
    "        \n",
    "        # Calculate the NDCG\n",
    "        relevance_scores = np.zeros(11)\n",
    "        relevance_scores[0] = 1  # The first item is the ground truth (relevant) item\n",
    "        ndcg_score = ndcg_at_k(relevance_scores[top10_indices.cpu().numpy()], 10)\n",
    "        ndcgs.append(ndcg_score)\n",
    "\n",
    "print(\"The Hit Rate@5 is {:.3f}\".format(np.average(hits_5)))\n",
    "print(\"The Hit Rate@10 is {:.3f}\".format(np.average(hits_10)))\n",
    "print(\"The NDCG@10 is {:.3f}\".format(np.average(ndcgs)))\n",
    "print(\"The Recall is {:.3f}\".format(np.average(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90336d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
