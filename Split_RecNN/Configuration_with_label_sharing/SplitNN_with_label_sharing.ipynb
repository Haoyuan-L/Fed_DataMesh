{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabc0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "# np.random.seed(666)\n",
    "from Distributed_HM_Data import HMSaleTrainDataLoader, Distributed_HM, binary_acc\n",
    "from utils_models import SalesNN, CustomersNN, ProductsNN\n",
    "\n",
    "dataDir = Path.cwd().parent.parent/'Data/'\n",
    "\n",
    "# model will train on CPU since PySyft 0.2.9 exist bugs with CUDA\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3df47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "# preserve training log\n",
    "so = open(\"config1.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f92facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesNN(nn.Module):\n",
    "    \"\"\" Partial model for sales domain\n",
    "    Args:\n",
    "        num_users (int): Number of users\n",
    "        num_items (int): Number of products\n",
    "        prices (float): price of transactions\n",
    "        sales_channels (float): sales channels\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_users: int, \n",
    "            num_items: int,\n",
    "            user_embedding_dim: int = 32,\n",
    "            item_embedding_dim: int = 64,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.user_embedding_layer = nn.Embedding(num_embeddings=num_users, embedding_dim=user_embedding_dim)\n",
    "        self.item_embedding_layer = nn.Embedding(num_embeddings=num_items, embedding_dim=item_embedding_dim)\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_embedding_layer(user_input)\n",
    "        item_embedding = self.item_embedding_layer(item_input)\n",
    "\n",
    "        latent_vec = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "\n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "    \n",
    "class CustomersNN(nn.Module):\n",
    "    \"\"\" Partial model for customer domain\n",
    "    Args:\n",
    "        club_status (int): active or inactive customers' status\n",
    "        age_groups (int): age of customers\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 2,\n",
    "            output_size: int = 4,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [output_size]\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, club_status, age_groups):\n",
    "        \n",
    "        latent_vec = torch.cat([club_status, age_groups], dim=-1)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            latent_vec = layer(latent_vec)\n",
    "        \n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class ProductsNN(nn.Module):\n",
    "    \"\"\" Partial model for product domain\n",
    "    Args:\n",
    "        num_product_groups (int): Number of product groups\n",
    "        num_color_groups: (int): Number of color groups\n",
    "        num_index_name: (int): Number of index name\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_product_groups: int,\n",
    "            num_color_groups: int,\n",
    "            num_index_name: int,\n",
    "            product_group_embedding_dim: int = 5,\n",
    "            color_group_embedding_dim: int = 16,\n",
    "            index_name_embedding_dim: int = 3,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.product_group_embedding_layer = nn.Embedding(num_embeddings=num_product_groups, embedding_dim=product_group_embedding_dim)\n",
    "        self.color_group_embedding_layer = nn.Embedding(num_embeddings=num_color_groups, embedding_dim=color_group_embedding_dim)\n",
    "        self.index_name_embedding_layer = nn.Embedding(num_embeddings=num_index_name, embedding_dim=index_name_embedding_dim)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, product_groups, color_groups, index_name):\n",
    "        product_group_embedding = self.product_group_embedding_layer(product_groups)\n",
    "        color_group_embedding = self.color_group_embedding_layer(color_groups)\n",
    "        index_name_embedding = self.index_name_embedding_layer(index_name)\n",
    "\n",
    "        latent_vec = torch.cat([product_group_embedding, color_group_embedding, index_name_embedding], dim=-1)\n",
    "        \n",
    "        return torch.squeeze(latent_vec)\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fc3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized models for specific configuration\n",
    "\n",
    "class GovernanceNN(nn.Module):\n",
    "    \"\"\" Partial model for goverance side\n",
    "    Args:\n",
    "        agg_latent_input (int): aggregated input of latent vectors from client models\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 124,\n",
    "            hidden_size_1: int = 256,\n",
    "            hidden_size_2: int = 128,\n",
    "            output_size: int = 1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        in_channels = (\n",
    "            [input_size] \n",
    "            + [hidden_size_1]\n",
    "            + [hidden_size_2]\n",
    "            + [hidden_size_2]\n",
    "            + [output_size]\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[nn.Linear(in_features=in_channels[i], out_features=in_channels[i+1]) for i in range(len(in_channels)-1) if i != len(in_channels)-1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, agg_latent_input):\n",
    "        \n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            agg_latent_input = layer(agg_latent_input)\n",
    "            agg_latent_input = self.relu(agg_latent_input)\n",
    "        out = agg_latent_input\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "\n",
    "class SplitNN(nn.Module):\n",
    "    def __init__(self, models, optimizers, data_owner, server):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        self.data_owners = data_owner\n",
    "        self.server = server\n",
    "#         self.outputs = [None]*len(self.models)\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, data_pointer):\n",
    "        \n",
    "        #individual client's output upto their respective cut layer\n",
    "        client_output = {}\n",
    "\n",
    "        #outputs that is moved to server and subjected to concatenate for server input\n",
    "        remote_output = []\n",
    "        \n",
    "        for owner in self.data_owners:\n",
    "            if owner.id == \"sales_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"customer_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "            elif owner.id == \"product_domain\":\n",
    "                client_output[owner.id] = self.models[owner.id](data_pointer[owner.id][0], data_pointer[owner.id][1], data_pointer[owner.id][2])\n",
    "                remote_output.append(\n",
    "                    client_output[owner.id].move(self.server, requires_grad=True)\n",
    "                )\n",
    "        \n",
    "        # concat outputs from clients and send to server side\n",
    "        server_input = torch.cat(remote_output, dim=-1)\n",
    "        # make prediction on server model\n",
    "        pred = self.models[\"server\"](server_input)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "    \n",
    "    def train(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].train()\n",
    "#             if loc == \"server\":\n",
    "#                 for i in range(len(self.models[loc])):\n",
    "#                     self.models[loc][i].train()\n",
    "#             else:\n",
    "#                 self.models[loc].train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].eval()        \n",
    "            \n",
    "    @property\n",
    "    def location(self):\n",
    "        return self.models[0].location if self.models and len(self.models) else None\n",
    "    \n",
    "def train(x, target, splitNN):\n",
    "    # set up bias weight for negative sampling\n",
    "    weights = torch.tensor([1.0, 3.0])\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    # make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1])\n",
    "    loss = criterion(pred, target.float())\n",
    "    \n",
    "    # backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss.detach().get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521f60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712250cb9c22497781f179d7e3ef1069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/293769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 10346\n",
      "num_items: 15516\n",
      "num_product_groups: 11\n",
      "num_color_groups: 48\n",
      "num_index_name: 9\n"
     ]
    }
   ],
   "source": [
    "hm_data = pd.read_csv(dataDir/'train_transactions_large.csv')\n",
    "hm_test_data = pd.read_csv(dataDir/'test_transactions_large.csv')\n",
    "article_data = pd.read_csv(dataDir/'article_data_large.csv')\n",
    "all_products_id = article_data[\"article_id\"].unique()\n",
    "transactions = pd.concat([hm_data, hm_test_data], ignore_index=True)\n",
    "\n",
    "customer_product_set = set(zip(transactions[\"customer_id\"], transactions[\"article_id\"], \n",
    "                               transactions[\"club_member_status\"], transactions[\"age\"], \n",
    "                               transactions[\"product_group_name\"], transactions[\"colour_group_name\"], transactions[\"index_name\"]))\n",
    "\n",
    "train_data = HMSaleTrainDataLoader(hm_data, all_products_id, customer_product_set)\n",
    "train_loader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "# set up virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "sales_domain = sy.VirtualWorker(hook, id=\"sales_domain\")\n",
    "customer_domain = sy.VirtualWorker(hook, id=\"customer_domain\")\n",
    "product_domain = sy.VirtualWorker(hook, id=\"product_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (sales_domain, customer_domain, product_domain)\n",
    "model_locations = [sales_domain, customer_domain, product_domain, server]\n",
    "\n",
    "distributed_trainloader = Distributed_HM(data_owners=data_owners, data_loader=train_loader)\n",
    "\n",
    "# set up parameters for model\n",
    "num_users = len(hm_data.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "num_product_groups = len(hm_data.product_group_name.unique())\n",
    "print(\"num_product_groups:\", num_product_groups)\n",
    "num_color_groups = len(hm_data.colour_group_name.unique())\n",
    "print(\"num_color_groups:\", num_color_groups)\n",
    "num_index_name = len(hm_data.index_name.unique())\n",
    "print(\"num_index_name:\", num_index_name)\n",
    "\n",
    "models = {\n",
    "    \"sales_domain\": SalesNN(num_users=num_users, num_items=num_items),\n",
    "    \"customer_domain\": CustomersNN(),\n",
    "    \"product_domain\": ProductsNN(num_product_groups=num_product_groups, num_color_groups=num_color_groups, num_index_name=num_index_name),\n",
    "    \"server\": GovernanceNN(),\n",
    "}\n",
    "\n",
    "# set up optimizer for clients' model\n",
    "\n",
    "optimizers = [\n",
    "    optim.Adam(models[location.id].parameters(), lr=0.005)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392a0423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sales_domain': SalesNN(\n",
      "  (user_embedding_layer): Embedding(10346, 32)\n",
      "  (item_embedding_layer): Embedding(15516, 64)\n",
      "), 'customer_domain': CustomersNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "  )\n",
      "), 'product_domain': ProductsNN(\n",
      "  (product_group_embedding_layer): Embedding(11, 5)\n",
      "  (color_group_embedding_layer): Embedding(48, 16)\n",
      "  (index_name_embedding_layer): Embedding(9, 3)\n",
      "), 'server': GovernanceNN(\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")}\n",
      "Epoch 0 - Training loss: 0.7660925984382629\n",
      "Epoch 1 - Training loss: 0.3917459547519684\n",
      "Epoch 2 - Training loss: 0.24729256331920624\n",
      "Epoch 3 - Training loss: 0.16989195346832275\n",
      "Epoch 4 - Training loss: 0.12983299791812897\n",
      "Epoch 5 - Training loss: 0.10355343669652939\n",
      "Epoch 6 - Training loss: 0.09211446344852448\n",
      "Epoch 7 - Training loss: 0.08180132508277893\n",
      "Epoch 8 - Training loss: 0.07549875974655151\n",
      "Epoch 9 - Training loss: 0.0685068815946579\n",
      "Epoch 10 - Training loss: 0.06248839199542999\n",
      "Epoch 11 - Training loss: 0.0568554513156414\n",
      "Epoch 12 - Training loss: 0.05009238421916962\n",
      "Epoch 13 - Training loss: 0.04670416936278343\n",
      "Epoch 14 - Training loss: 0.041677478700876236\n",
      "Epoch 15 - Training loss: 0.036971356719732285\n",
      "Epoch 16 - Training loss: 0.03117486648261547\n",
      "Epoch 17 - Training loss: 0.028710877522826195\n",
      "Epoch 18 - Training loss: 0.0268414244055748\n",
      "Epoch 19 - Training loss: 0.022119317203760147\n",
      "Epoch 20 - Training loss: 0.019467495381832123\n",
      "Epoch 21 - Training loss: 0.01934567466378212\n",
      "Epoch 22 - Training loss: 0.016566019505262375\n",
      "Epoch 23 - Training loss: 0.013078603893518448\n",
      "Epoch 24 - Training loss: 0.013774149119853973\n",
      "Epoch 25 - Training loss: 0.013393309898674488\n",
      "Epoch 26 - Training loss: 0.010432962328195572\n",
      "Epoch 27 - Training loss: 0.010630091652274132\n",
      "Epoch 28 - Training loss: 0.009585143066942692\n",
      "Epoch 29 - Training loss: 0.008967769332230091\n",
      "Epoch 30 - Training loss: 0.011973805725574493\n",
      "Epoch 31 - Training loss: 0.009653211571276188\n",
      "Epoch 32 - Training loss: 0.007404481992125511\n",
      "Epoch 33 - Training loss: 0.008589711040258408\n",
      "Epoch 34 - Training loss: 0.006822735536843538\n",
      "Epoch 35 - Training loss: 0.006578520871698856\n",
      "Epoch 36 - Training loss: 0.0077178278006613255\n",
      "Epoch 37 - Training loss: 0.006217935588210821\n",
      "Epoch 38 - Training loss: 0.007039402611553669\n",
      "Epoch 39 - Training loss: 0.004649707116186619\n",
      "Epoch 40 - Training loss: 0.01029913779348135\n",
      "Epoch 41 - Training loss: 0.005337459035217762\n",
      "Epoch 42 - Training loss: 0.004216564353555441\n",
      "Epoch 43 - Training loss: 0.0023199552670121193\n",
      "Epoch 44 - Training loss: 0.008413069881498814\n",
      "Epoch 45 - Training loss: 0.00710203917697072\n",
      "Epoch 46 - Training loss: 0.003278907621279359\n",
      "Epoch 47 - Training loss: 0.004595351871103048\n",
      "Epoch 48 - Training loss: 0.005001065321266651\n",
      "Epoch 49 - Training loss: 0.005815059412270784\n",
      "Epoch 50 - Training loss: 0.005539681296795607\n",
      "Epoch 51 - Training loss: 0.0051488750614225864\n",
      "Epoch 52 - Training loss: 0.004975366871803999\n",
      "Epoch 53 - Training loss: 0.004908544011414051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_198364/504512500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributed_trainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_198364/1470644644.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, target, splitNN)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0msplitNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_198364/1470644644.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/generic/frameworks/hook/pointers.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"send message to worker location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# For backwards compatibility with Udacity course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# Step 3: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbin_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/serde.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(obj, worker, simplified, force_full_simplification, strategy)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsgpack_serialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_full_simplification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(obj, worker, simplified, force_full_simplification)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_serialize_msgpack_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_full_simplification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_serialize_msgpack_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36m_serialize_msgpack_binary\u001b[0;34m(simple_objects, worker, simplified, force_full_simplification)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;31m# even if compressed flag is set to false by the caller we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# output the input stream as it is with header set to '0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/compression.py\u001b[0m in \u001b[0;36m_compress\u001b[0;34m(decompressed_input_bin)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mcompress_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress_scheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_compress_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheme_to_bytes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompress_scheme\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompress_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/compression.py\u001b[0m in \u001b[0;36m_apply_compress_scheme\u001b[0;34m(decompressed_input_bin)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdecompressed_input_bin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcompressed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscheme_to_compression\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_compress_scheme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datamesh-dev/lib/python3.7/site-packages/syft/serde/compression.py\u001b[0m in \u001b[0;36mapply_lz4_compression\u001b[0;34m(decompressed_input_bin)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompressed_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLZ4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLZ4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(models)\n",
    "\n",
    "epochs = 200\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "splitnn = SplitNN(models, optimizers, data_owners, server)\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    splitnn.train()\n",
    "    for data_ptr, labels in distributed_trainloader:  \n",
    "        labels = labels.send(server)\n",
    "        loss = train(data_ptr, labels, splitnn)\n",
    "        running_loss += loss\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(distributed_trainloader)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c31a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "         torch.save(models[loc].get().state_dict(), f\"{file_prefix}_{loc}_weights.pth\")\n",
    "    \n",
    "save_weights(models, \"Split_RecNN_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b6ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "        model_weights = torch.load(f\"{file_prefix}_{loc}_weights.pth\")\n",
    "        models[loc].load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719cfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    acc = 0.0\n",
    "    \n",
    "    y_pred_label = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_pred = (y_pred_label == y_test).sum()\n",
    "    acc = correct_pred.item()/y_test.shape[0]\n",
    "    return acc\n",
    "\n",
    "def predict(splitnn, data_ptr, product, test_items):\n",
    "    hits_10 = 1\n",
    "    hits_5 = 1\n",
    "    \n",
    "    y_pred = splitnn.forward(data_ptr).get()\n",
    "#     y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred = y_pred.squeeze()\n",
    "    \n",
    "    top10_probs, top10_indices = torch.topk(torch.sigmoid(y_pred), 10)\n",
    "    # Convert the top 10 probabilities to a list\n",
    "    top10_items = [test_items[i].item() for i in top10_indices]\n",
    "    if product in top10_items:\n",
    "        hits_10 = 1\n",
    "    else:\n",
    "        hits_10 = 0\n",
    "    \n",
    "    top5_probs, topk_indices = torch.topk(torch.sigmoid(y_pred), 5)\n",
    "    # Convert the top 5 probabilities to a list\n",
    "    top5_items = [test_items[i].item() for i in topk_indices]\n",
    "    if product in top5_items:\n",
    "        hits_5 = 1\n",
    "    else:\n",
    "        hits_5 = 0\n",
    "    \n",
    "    return hits_5, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c049fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum((2**r - 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "198f3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distributed_HM_test(Dataset):\n",
    "    def __init__(self, data_owners, customer_batch, product_batch, club_status_batch, age_groups_batch, product_groups_batch, color_groups_batch, index_name_batch, label_batch):\n",
    "        self.data_owners = data_owners\n",
    "        self.no_of_owner = len(data_owners)\n",
    "        self.customer_batch = customer_batch\n",
    "        self.product_batch = product_batch\n",
    "        self.club_status_batch = club_status_batch\n",
    "        self.age_groups_batch = age_groups_batch\n",
    "        self.product_groups_batch = product_groups_batch\n",
    "        self.color_groups_batch = color_groups_batch\n",
    "        self.index_name_batch = index_name_batch\n",
    "        self.label_batch = label_batch\n",
    "\n",
    "        self.data_pointer = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.curr_data_dict = {}\n",
    "        self.labels.append(label_batch.reshape(-1, 1))\n",
    "\n",
    "        # split data batch based on domains\n",
    "        sales_domain = [customer_batch, product_batch]\n",
    "        customer_domain = [club_status_batch.float(), age_groups_batch.float()]\n",
    "        product_domain = [product_groups_batch, color_groups_batch, index_name_batch]\n",
    "            \n",
    "        # set data owners for each domain team\n",
    "        sales_owner = self.data_owners[0]\n",
    "        customer_owner = self.data_owners[1]\n",
    "        product_owner = self.data_owners[2]\n",
    "            \n",
    "            \n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_domain:\n",
    "            sales_part_ptr.append(tensor.send(sales_owner))\n",
    "        self.curr_data_dict[sales_owner.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_domain:\n",
    "            customer_part_ptr.append(tensor.send(customer_owner))\n",
    "        self.curr_data_dict[customer_owner.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_domain:\n",
    "            product_part_ptr.append(tensor.send(product_owner))\n",
    "        self.curr_data_dict[product_owner.id] = product_part_ptr\n",
    "\n",
    "        self.data_pointer.append(self.curr_data_dict)\n",
    "            \n",
    "    def get_data(self):\n",
    "        return self.data_pointer[0], self.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54080f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 10346\n",
      "num_items: 15516\n",
      "num_product_groups: 11\n",
      "num_color_groups: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# model test\n",
    "train_transactions = pd.read_csv(dataDir/'train_transactions_large.csv')\n",
    "test_transactions = pd.read_csv(dataDir/'test_transactions_large.csv')\n",
    "article_data = pd.read_csv(dataDir/'article_data_large.csv')\n",
    "customer_data = pd.read_csv(dataDir/'customer_data_large.csv')\n",
    "\n",
    "all_products_id = article_data[\"article_id\"].unique()\n",
    "customer_product_test = set(zip(test_transactions[\"customer_id\"], test_transactions[\"article_id\"], \n",
    "                                test_transactions[\"club_member_status\"], test_transactions[\"age\"], \n",
    "                                test_transactions[\"product_group_name\"], test_transactions[\"colour_group_name\"], \n",
    "                                test_transactions[\"index_name\"]))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = train_transactions.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "# set up parameters for model\n",
    "num_users = len(train_transactions.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "num_product_groups = len(article_data.product_group_name.unique())\n",
    "print(\"num_product_groups:\", num_product_groups)\n",
    "num_color_groups = len(article_data.colour_group_name.unique())\n",
    "print(\"num_color_groups:\", num_color_groups)\n",
    "num_index_name = len(article_data.index_name.unique())\n",
    "\n",
    "# load the model parameters\n",
    "models = {\n",
    "    \"sales_domain\": SalesNN(num_users=num_users, num_items=num_items),\n",
    "    \"customer_domain\": CustomersNN(),\n",
    "    \"product_domain\": ProductsNN(num_product_groups=num_product_groups, num_color_groups=num_color_groups, num_index_name=num_index_name),\n",
    "    \"server\": GovernanceNN(),\n",
    "}\n",
    "\n",
    "load_weights(models, \"Split_RecNN_large\")\n",
    "\n",
    "# set up virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "sales_domain = sy.VirtualWorker(hook, id=\"sales_domain\")\n",
    "customer_domain = sy.VirtualWorker(hook, id=\"customer_domain\")\n",
    "product_domain = sy.VirtualWorker(hook, id=\"product_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (sales_domain, customer_domain, product_domain)\n",
    "model_locations = [sales_domain, customer_domain, product_domain, server]\n",
    "\n",
    "# set up optimizer for clients' model\n",
    "optimizers = [\n",
    "    optim.Adam(models[location.id].parameters(), lr=0.005)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)\n",
    "\n",
    "# Load the weights locally\n",
    "splitnn = SplitNN(models, optimizers, data_owners, server)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6765986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2f1b48cf264fd0883577bc1649442d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Rate@5 is 0.441\n",
      "The Hit Rate@10 is 0.897\n",
      "The NDCG@10 is 0.405\n",
      "The Recall is 0.956\n"
     ]
    }
   ],
   "source": [
    "# model test\n",
    "# np.random.seed(28)\n",
    "splitnn.eval()\n",
    "hits = []\n",
    "pred_prob = []\n",
    "acc = []\n",
    "hits_10 = []\n",
    "hits_5 = []\n",
    "recall = []\n",
    "ndcgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for customer, product, club_status, age_groups, product_groups, color_groups, index_name in tqdm(customer_product_test):\n",
    "        \n",
    "        # select 10 products from item set that customer has no interactions\n",
    "        interacted_items = user_interacted_items[customer]\n",
    "        interacted_items = interacted_items + [product]\n",
    "        not_interacted_items = set(all_products_id) - set(interacted_items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 10, replace=False))\n",
    "        test_items = [product] + selected_not_interacted\n",
    "        \n",
    "        # get the other product features based on the selected test items\n",
    "        product_groups_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"product_group_name\"].to_numpy()).reshape(-1, 1)\n",
    "        color_groups_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"colour_group_name\"].to_numpy()).reshape(-1, 1)\n",
    "        index_name_batch = torch.tensor(article_data.loc[article_data[\"article_id\"].isin(test_items)][\"index_name\"].to_numpy()).reshape(-1, 1)\n",
    "\n",
    "        test_items = torch.tensor(test_items).reshape(-1, 1)\n",
    "        customer_batch = torch.tensor([customer]*11).reshape(-1, 1)\n",
    "        club_status_batch = torch.tensor([club_status]*11).reshape(-1, 1)\n",
    "        age_groups_batch = torch.tensor([age_groups]*11).reshape(-1, 1)\n",
    "#         label_batch = torch.tensor([1]+[0]*10).reshape(-1, 1)\n",
    "        \n",
    "        # batch prediction on test items\n",
    "        batch_data_dict = {}\n",
    "        # split data batch based on domains\n",
    "        sales_batch = [customer_batch, test_items]\n",
    "        customer_batch = [club_status_batch.float(), age_groups_batch.float()]\n",
    "        product_batch = [product_groups_batch, color_groups_batch, index_name_batch]\n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_batch:\n",
    "            sales_part_ptr.append(tensor.send(sales_domain))\n",
    "        batch_data_dict[sales_domain.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_batch:\n",
    "            customer_part_ptr.append(tensor.send(customer_domain))\n",
    "        batch_data_dict[customer_domain.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_batch:\n",
    "            product_part_ptr.append(tensor.send(product_domain))\n",
    "        batch_data_dict[product_domain.id] = product_part_ptr\n",
    "        \n",
    "        y_pred = splitnn.forward(batch_data_dict).get()\n",
    "#         y_pred = torch.sigmoid(y_pred)\n",
    "        y_pred = y_pred.squeeze()\n",
    "\n",
    "        top10_probs, top10_indices = torch.topk(y_pred, 10)\n",
    "        # Convert the top 10 probabilities to a list\n",
    "        top10_items = [test_items[i].item() for i in top10_indices]\n",
    "        if product in top10_items:\n",
    "            hits_10.append(1)\n",
    "        else:\n",
    "            hits_10.append(0)\n",
    "\n",
    "        top5_probs, top5_indices = torch.topk(y_pred, 5)\n",
    "        # Convert the top 5 probabilities to a list\n",
    "        top5_items = [test_items[i].item() for i in top5_indices]\n",
    "        if product in top5_items:\n",
    "            hits_5.append(1)\n",
    "        else:\n",
    "            hits_5.append(0)\n",
    "    \n",
    "        \n",
    "        # single prediction on product\n",
    "        # send split data to VirtualWorkers and add the data pointer to the dict\n",
    "        single_data_ptr = {}\n",
    "        sales_data = [torch.tensor([customer]).reshape(-1, 1), torch.tensor([product]).reshape(-1, 1)]\n",
    "        customer_data = [torch.tensor([club_status]).reshape(-1, 1).float(), torch.tensor([age_groups]).reshape(-1, 1).float()]\n",
    "        product_data = [torch.tensor([product_groups]).reshape(-1, 1), torch.tensor([color_groups]).reshape(-1, 1), torch.tensor([index_name]).reshape(-1, 1)]\n",
    "        \n",
    "        sales_part_ptr = []\n",
    "        for tensor in sales_data:\n",
    "            sales_part_ptr.append(tensor.send(sales_domain))\n",
    "        single_data_ptr[sales_domain.id] = sales_part_ptr\n",
    "            \n",
    "        customer_part_ptr = []\n",
    "        for tensor in customer_data:\n",
    "            customer_part_ptr.append(tensor.send(customer_domain))\n",
    "        single_data_ptr[customer_domain.id] = customer_part_ptr\n",
    "            \n",
    "        product_part_ptr = []\n",
    "        for tensor in product_data:\n",
    "            product_part_ptr.append(tensor.send(product_domain))\n",
    "        single_data_ptr[product_domain.id] = product_part_ptr\n",
    "        \n",
    "        y_single_pred = splitnn.forward(single_data_ptr).get()\n",
    "        if torch.round(torch.sigmoid(y_single_pred)) == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)\n",
    "        \n",
    "        # Calculate the NDCG\n",
    "        relevance_scores = np.zeros(11)\n",
    "        relevance_scores[0] = 1  # The first item is the ground truth (relevant) item\n",
    "        ndcg_score = ndcg_at_k(relevance_scores[top10_indices.cpu().numpy()], 10)\n",
    "        ndcgs.append(ndcg_score)\n",
    "\n",
    "print(\"The Hit Rate@5 is {:.3f}\".format(np.average(hits_5)))\n",
    "print(\"The Hit Rate@10 is {:.3f}\".format(np.average(hits_10)))\n",
    "print(\"The NDCG@10 is {:.3f}\".format(np.average(ndcgs)))\n",
    "print(\"The Recall is {:.3f}\".format(np.average(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90336d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
