{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "np.random.seed(666)\n",
    "from Distributed_CreditCard_Data import CreditCardDataLoader, Distributed_CreditCard, binary_acc\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataDir = Path.cwd().parent.parent/'CreditCard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "# preserve training log\n",
    "so = open(\"config1.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalNN(nn.Module):\n",
    "    \"\"\" Partial model for digital transaction domain\n",
    "    Args:\n",
    "        input_size (int): number of features in digital transaction domain\n",
    "        digital_transaction_intput (tensor): input size of digital transaction domain\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size: int,\n",
    "            hidden_size: int = 32,\n",
    "            output_size: int = 16,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, digital_input):\n",
    "        \n",
    "        for layer in self.layers_stack:\n",
    "            digital_input = layer(digital_input)\n",
    "        \n",
    "        return digital_input\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class RetailNN(nn.Module):\n",
    "    \"\"\" Partial model for retail transaction domain\n",
    "\n",
    "    Args:\n",
    "        input_size (int): number of features in retail transaction domain\n",
    "        retail_transaction_intput (tensor): input size of retail transaction domain\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size: int,\n",
    "            hidden_size: int = 32,\n",
    "            output_size: int = 16,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, retail_input):\n",
    "        \n",
    "        for layer in self.layers_stack:\n",
    "            retail_input = layer(retail_input)\n",
    "        \n",
    "        return retail_input\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "    \n",
    "class FraudPrevNN(nn.Module):\n",
    "    \"\"\" Partial model for fraud prevention domain\n",
    "\n",
    "    Args:\n",
    "        input_size (int): number of features in fraud prevention domain\n",
    "        fraud_prev_intput (tensor): input size of fraud prevention domain\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size: int,\n",
    "            hidden_size: int = 32,\n",
    "            output_size: int = 16,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, fraud_prev_input):\n",
    "        \n",
    "        for layer in self.layers_stack:\n",
    "            fraud_prev_input = layer(fraud_prev_input)\n",
    "        \n",
    "        return fraud_prev_input\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "class GovernanceNN(nn.Module):\n",
    "    \"\"\"Partial model for governance side\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): number of features in governance side\n",
    "        governance_input (tensor): input size of governance side\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int = 48,\n",
    "            hidden_size: int = 32,\n",
    "            output_size: int = 2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, governance_input):\n",
    "\n",
    "        for layer in self.layers_stack:\n",
    "            governance_input = layer(governance_input)\n",
    "        return governance_input\n",
    "    \n",
    "    # save weights of partial model on remote worker\n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "    \n",
    "class SplitFraudNN(nn.Module):\n",
    "    \"\"\" Split fraud neural network \n",
    "    \n",
    "    Args:\n",
    "        data_pointer (dict): syft data pointer\n",
    "    \"\"\"\n",
    "    def __init__(self, models, optimizers, data_owners, server):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        self.data_owners = data_owners\n",
    "        self.server = server\n",
    "    \n",
    "    def forward(self, data_pointer):\n",
    "\n",
    "        #individual client's output upto their respective cut layer\n",
    "        client_output = {}\n",
    "\n",
    "        #outputs that is moved to server and subjected to concatenate for server input\n",
    "        remote_output = []\n",
    "\n",
    "        for owner in self.data_owners:\n",
    "            client_output[owner.id] = self.models[owner.id](data_pointer[owner.id])\n",
    "            remote_output.append(client_output[owner.id].move(self.server, requires_grad=True))\n",
    "        \n",
    "        # concatenate the output of individual client's output\n",
    "        server_input = torch.cat(remote_output, dim=1)\n",
    "        # server side make prediction\n",
    "        pred = self.models[\"server\"](server_input)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "    \n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "\n",
    "    def train(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for loc in self.models.keys():\n",
    "            self.models[loc].eval()\n",
    "\n",
    "def train(x, target, splitNN):\n",
    "    \n",
    "    splitNN.zero_grads()\n",
    "    pred = splitNN.forward(x)\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    #Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss.detach().get()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_transaction_data = np.load(dataDir/'digital_transaction_train.npy')\n",
    "retail_transaction_data = np.load(dataDir/'retail_transaction_train.npy')\n",
    "fraud_prevention_data = np.load(dataDir/'fraud_prevention_train.npy')\n",
    "labels = np.load(dataDir/'labels_train.npy')\n",
    "\n",
    "train_data = CreditCardDataLoader(digital_transaction_data, retail_transaction_data, fraud_prevention_data, labels)\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "\n",
    "# set up virtual workers for SplitFraudNN\n",
    "hook = sy.TorchHook(torch)\n",
    "digital_domain = sy.VirtualWorker(hook, id=\"digital_domain\")\n",
    "retail_domain = sy.VirtualWorker(hook, id=\"retail_domain\")\n",
    "fraud_prevention_domain = sy.VirtualWorker(hook, id=\"fraud_prevention_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (digital_domain, retail_domain, fraud_prevention_domain)\n",
    "model_locations = [digital_domain, retail_domain, fraud_prevention_domain, server]\n",
    "\n",
    "# set up distributed data loader for SplitFraudNN\n",
    "distributed_trainloader = Distributed_CreditCard(data_owners=data_owners, data_loader=train_loader)\n",
    "\n",
    "# set up models for SplitFraudNN\n",
    "models = {\n",
    "    \"digital_domain\": DigitalNN(input_size=digital_transaction_data.shape[1]),\n",
    "    \"retail_domain\": RetailNN(input_size=retail_transaction_data.shape[1]),\n",
    "    \"fraud_prevention_domain\": FraudPrevNN(input_size=fraud_prevention_data.shape[1]),\n",
    "    \"server\": GovernanceNN(input_size=48),\n",
    "}\n",
    "\n",
    "# set up optimizers for SplitFraudNN\n",
    "optimizers = [\n",
    "    optim.Adam(models[owner.id].parameters(), lr=1e-4) for owner in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'digital_domain': DigitalNN(\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "), 'retail_domain': RetailNN(\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "), 'fraud_prevention_domain': FraudPrevNN(\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "), 'server': GovernanceNN(\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f3e63698244136afd7eb6ba6c6dee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(models)\n",
    "\n",
    "epochs = 30\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "splitFraudNN = SplitFraudNN(models, optimizers, data_owners, server)\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    running_loss = 0.0\n",
    "    splitFraudNN.train()\n",
    "    for data_ptr, labels in distributed_trainloader:\n",
    "        labels = labels.send(server)\n",
    "        loss = train(data_ptr, labels, splitFraudNN)\n",
    "        running_loss += loss\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(distributed_trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    acc = 0.0\n",
    "    \n",
    "    y_pred_label = torch.softmax(y_pred, dim=1)\n",
    "    _, y_pred_label = torch.max(y_pred_label, dim=1)\n",
    "    correct_pred = (y_pred_label == y_test).sum()\n",
    "    acc = correct_pred.item()/y_test.shape[0]\n",
    "    return acc\n",
    "\n",
    "def predict(models, dataloader, dataset_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data_ptr, label in dataloader:\n",
    "            output = splitFraudNN.forward(data_ptr).get()\n",
    "            acc = binary_acc(output, label)\n",
    "            test_acc += acc\n",
    "            y_pred_label = torch.softmax(output, dim=1)\n",
    "            _, y_pred_label = torch.max(y_pred_label, dim=1)\n",
    "            y_pred.extend(y_pred_label.tolist())\n",
    "            y_true.extend(label.long().tolist())\n",
    "            \n",
    "    print(\"Accuracy on dataset {} is ({:.3}%)\".format(dataset_name, 100* test_acc/len(dataloader)))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(\"ROC AUC Score: \", roc_auc)\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model test \"\"\"\n",
    "digital_transaction_test = np.load(dataDir/'digital_transaction_test.npy')\n",
    "retail_transaction_test = np.load(dataDir/'retail_transaction_test.npy')\n",
    "fraud_prevention_test = np.load(dataDir/'fraud_prevention_test.npy')\n",
    "labels_test = np.load(dataDir/'labels_test.npy')\n",
    "\n",
    "test_data = CreditCardDataLoader(digital_transaction_test, retail_transaction_test, fraud_prevention_test, labels_test)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "distributed_testloader = Distributed_CreditCard(data_owners=data_owners, data_loader=test_loader)\n",
    "\n",
    "#Accuracy on train and test sets\n",
    "splitFraudNN.eval()\n",
    "# predict(models, distributed_trainloader, \"Train set\")\n",
    "y_true, y_pred = predict(models, distributed_testloader, \"Test set\")\n",
    "\n",
    "class_def = {0 : \"Normal\", 1 : \"Fraudulent\"}\n",
    "cm_df = pd.DataFrame(confusion_matrix(y_true, y_pred)).rename(columns=class_def, index=class_def)\n",
    "cm_df = cm_df / np.sum(cm_df)\n",
    "sns.heatmap(cm_df, annot=True, fmt='0.2%', cmap=\"YlGnBu\").set(title=\"Confusion Matrix\", xlabel=\"Predicted Label\", ylabel=\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "         torch.save(models[loc].get().state_dict(), f\"{file_prefix}_{loc}_weights.pth\")\n",
    "\n",
    "save_weights(models, \"Split_FraudNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  load trained model for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(models, file_prefix):\n",
    "    for loc in models.keys():\n",
    "        model_weights = torch.load(f\"{file_prefix}_{loc}_weights.pth\")\n",
    "        models[loc].load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "digital_transaction_test = np.load(dataDir/'digital_transaction_test.npy')\n",
    "retail_transaction_test = np.load(dataDir/'retail_transaction_test.npy')\n",
    "fraud_prevention_test = np.load(dataDir/'fraud_prevention_test.npy')\n",
    "labels_test = np.load(dataDir/'labels_test.npy')\n",
    "\n",
    "test_data = CreditCardDataLoader(digital_transaction_test, retail_transaction_test, fraud_prevention_test, labels_test)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# set up models for SplitFraudNN\n",
    "models = {\n",
    "    \"digital_domain\": DigitalNN(input_size=digital_transaction_data.shape[1]),\n",
    "    \"retail_domain\": RetailNN(input_size=retail_transaction_data.shape[1]),\n",
    "    \"fraud_prevention_domain\": FraudPrevNN(input_size=fraud_prevention_data.shape[1]),\n",
    "    \"server\": GovernanceNN(input_size=48),\n",
    "}\n",
    "\n",
    "# Load the weights locally\n",
    "load_weights(models, \"Split_FraudNN\")\n",
    "\n",
    "# set up virtual workers for SplitFraudNN\n",
    "hook = sy.TorchHook(torch)\n",
    "digital_domain = sy.VirtualWorker(hook, id=\"digital_domain\")\n",
    "retail_domain = sy.VirtualWorker(hook, id=\"retail_domain\")\n",
    "fraud_prevention_domain = sy.VirtualWorker(hook, id=\"fraud_prevention_domain\")\n",
    "server = sy.VirtualWorker(hook, id=\"server\")\n",
    "\n",
    "data_owners = (digital_domain, retail_domain, fraud_prevention_domain)\n",
    "model_locations = [digital_domain, retail_domain, fraud_prevention_domain, server]\n",
    "\n",
    "# set up distributed data loader for SplitFraudNN\n",
    "distributed_testloader = Distributed_CreditCard(data_owners=data_owners, data_loader=test_loader)\n",
    "\n",
    "# set up optimizers for SplitFraudNN\n",
    "optimizers = [\n",
    "    optim.Adam(models[owner.id].parameters(), lr=1e-4) for owner in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)\n",
    "    \n",
    "splitFraudNN = SplitFraudNN(models, optimizers, data_owners, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset Test set is (99.9%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85292\n",
      "           1       0.68      0.86      0.76       148\n",
      "\n",
      "    accuracy                           1.00     85440\n",
      "   macro avg       0.84      0.93      0.88     85440\n",
      "weighted avg       1.00      1.00      1.00     85440\n",
      "\n",
      "ROC AUC Score:  0.9320748373473131\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on train and test sets\n",
    "splitFraudNN.eval()\n",
    "# predict(models, distributed_trainloader, \"Train set\")\n",
    "y_true, y_pred = predict(models, distributed_testloader, \"Test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
