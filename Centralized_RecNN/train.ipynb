{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "dataDir = Path.cwd().parent.parent.parent/'backup/HM_data'\n",
    "np.random.seed(66)\n",
    "\n",
    "transactions = pd.read_csv(dataDir/'transactions.csv')\n",
    "# articles = pd.read_csv(dataDir/'articles.csv')\n",
    "# customers = pd.read_csv(dataDir/'customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1362281"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transactions.customer_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92598 rows of data from 2668 users (users with suffication data)\n"
     ]
    }
   ],
   "source": [
    "# calculate customer interactions in transaction data\n",
    "# drop customers that only contain few interactions\n",
    "transactions[\"interaction\"] = 1\n",
    "transactions_temp = transactions.drop_duplicates(subset=[\"customer_id\", \"article_id\"])\n",
    "comb_transactions = transactions_temp[[\"customer_id\", \"interaction\"]].groupby(by=[\"customer_id\"], sort=False, as_index=False).sum([\"interaction\"])\n",
    "comb_transactions = comb_transactions.loc[comb_transactions.interaction >= 5]\n",
    "\n",
    "# randomly select part of the transaction data\n",
    "rand_userIds = np.random.choice(comb_transactions.customer_id.unique(), \n",
    "                                size=int(len(comb_transactions['customer_id'].unique())*0.003), \n",
    "                                replace=False)\n",
    "\n",
    "transactions = transactions.loc[transactions['customer_id'].isin(rand_userIds)]\n",
    "\n",
    "print('There are {} rows of data from {} users (users with suffication data)'.format(len(transactions), len(rand_userIds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>015e83393e4fc3b071ba6fc5f174bf3bf9813c88dcaf3e...</td>\n",
       "      <td>640244003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0277917f4faac280023baecd03c3790b951a9403085da1...</td>\n",
       "      <td>686269001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0277917f4faac280023baecd03c3790b951a9403085da1...</td>\n",
       "      <td>633130002</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>07a55454c1c21b932c594eb7ca3ca52ae208cbbcc1552c...</td>\n",
       "      <td>532954003</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>07a55454c1c21b932c594eb7ca3ca52ae208cbbcc1552c...</td>\n",
       "      <td>200182001</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31782166</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>cf61e83193fa7b1877fcb795e555150570082c7c2da055...</td>\n",
       "      <td>523489001</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31782167</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>cf61e83193fa7b1877fcb795e555150570082c7c2da055...</td>\n",
       "      <td>678942057</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31786935</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>f479d7e42d8415dd7cfd4f7ba79ab811ca1d77fa85f2f8...</td>\n",
       "      <td>869331006</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31786936</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>f479d7e42d8415dd7cfd4f7ba79ab811ca1d77fa85f2f8...</td>\n",
       "      <td>853881001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31787356</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>f7e3375f513618e3f1ab7a809b9a9d94c4a1bfc89c9f5c...</td>\n",
       "      <td>880597001</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79452 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               t_dat                                        customer_id  \\\n",
       "248       2018-09-20  015e83393e4fc3b071ba6fc5f174bf3bf9813c88dcaf3e...   \n",
       "498       2018-09-20  0277917f4faac280023baecd03c3790b951a9403085da1...   \n",
       "499       2018-09-20  0277917f4faac280023baecd03c3790b951a9403085da1...   \n",
       "1458      2018-09-20  07a55454c1c21b932c594eb7ca3ca52ae208cbbcc1552c...   \n",
       "1459      2018-09-20  07a55454c1c21b932c594eb7ca3ca52ae208cbbcc1552c...   \n",
       "...              ...                                                ...   \n",
       "31782166  2020-09-22  cf61e83193fa7b1877fcb795e555150570082c7c2da055...   \n",
       "31782167  2020-09-22  cf61e83193fa7b1877fcb795e555150570082c7c2da055...   \n",
       "31786935  2020-09-22  f479d7e42d8415dd7cfd4f7ba79ab811ca1d77fa85f2f8...   \n",
       "31786936  2020-09-22  f479d7e42d8415dd7cfd4f7ba79ab811ca1d77fa85f2f8...   \n",
       "31787356  2020-09-22  f7e3375f513618e3f1ab7a809b9a9d94c4a1bfc89c9f5c...   \n",
       "\n",
       "          article_id     price  sales_channel_id  interaction  \n",
       "248        640244003  0.033881                 1            1  \n",
       "498        686269001  0.016932                 2            1  \n",
       "499        633130002  0.016932                 2            1  \n",
       "1458       532954003  0.003373                 2            1  \n",
       "1459       200182001  0.013542                 2            1  \n",
       "...              ...       ...               ...          ...  \n",
       "31782166   523489001  0.001678                 1            1  \n",
       "31782167   678942057  0.016932                 1            1  \n",
       "31786935   869331006  0.030492                 1            1  \n",
       "31786936   853881001  0.016932                 1            1  \n",
       "31787356   880597001  0.011847                 1            1  \n",
       "\n",
       "[79452 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.drop_duplicates(subset=[\"customer_id\", \"article_id\"], keep=\"first\", inplace=True)\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>rank_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "248   2018-09-20            0           0  0.033881                 1   \n",
       "498   2018-09-20            1           1  0.016932                 2   \n",
       "499   2018-09-20            1           2  0.016932                 2   \n",
       "1458  2018-09-20            2           3  0.003373                 2   \n",
       "1459  2018-09-20            2           4  0.013542                 2   \n",
       "\n",
       "      interaction  rank_latest  \n",
       "248             1         25.0  \n",
       "498             1         29.0  \n",
       "499             1         30.0  \n",
       "1458            1         41.0  \n",
       "1459            1         42.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set and test set\n",
    "\n",
    "transactions['rank_latest'] = transactions.groupby(['customer_id'])['t_dat'].rank(method='first', ascending=False)\n",
    "\n",
    "train_transactions = transactions[transactions['rank_latest'] != 1]\n",
    "test_transactions = transactions[transactions['rank_latest'] == 1]\n",
    "\n",
    "# drop articles that do not exist in training set\n",
    "test_product_list = list(set(test_transactions.article_id.unique()) & set(train_transactions.article_id.unique()))\n",
    "test_transactions = test_transactions.loc[test_transactions['article_id'].isin(test_product_list)]\n",
    "\n",
    "\"\"\"reindex\"\"\"\n",
    "# map string type customer_id to int type\n",
    "customer_mapper = {}\n",
    "customer_keys = train_transactions.customer_id.unique()\n",
    "customer_values = list(range(len(train_transactions.customer_id.unique())))\n",
    "customer_mapper.update(zip(customer_keys, customer_values))\n",
    "\n",
    "# map string type article_id to int type\n",
    "product_mapper = {}\n",
    "product_keys = train_transactions.article_id.unique()\n",
    "product_values = list(range(len(train_transactions.article_id.unique())))\n",
    "product_mapper.update(zip(product_keys, product_values))\n",
    "\n",
    "train_transactions[\"customer_id\"] = train_transactions[\"customer_id\"].map(customer_mapper)\n",
    "train_transactions[\"article_id\"] = train_transactions[\"article_id\"].map(product_mapper)\n",
    "test_transactions[\"customer_id\"] = test_transactions[\"customer_id\"].map(customer_mapper)\n",
    "test_transactions[\"article_id\"] = test_transactions[\"article_id\"].map(product_mapper)\n",
    "\n",
    "# get a list of all articles id\n",
    "all_products_id = train_transactions[\"article_id\"].unique()\n",
    "\n",
    "train_transactions.head()\n",
    "\n",
    "# drop columns that we no longer need\n",
    "# train_transactions = train_transactions[['customer_id', 'article_id', 'price']]\n",
    "# test_transactions = test_transactions[['customer_id', 'article_id', 'price']]\n",
    "# comb_transactions = train_transactions.groupby(by=[\"customer_id\", \"article_id\"], sort=False, as_index=False).sum([\"interaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2668"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_transactions.customer_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def sparse_to_tensor(sparse_matrix):\n",
    "    \"\"\"\n",
    "    Transform scipy coo matrix to pytorch sparse tensor\n",
    "    \"\"\"\n",
    "    sparse_matrix = sparse_matrix.tocoo()\n",
    "    values = sparse_matrix.data\n",
    "    indices = (sparse_matrix.row, sparse_matrix.col) # np.vstack\n",
    "    shape = sparse_matrix.shape\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.DoubleTensor(values)\n",
    "    s = torch.Size(shape)\n",
    "\n",
    "    return torch.sparse.DoubleTensor(i, v, s)\n",
    "\n",
    "def sparse_batch_collate(batch): \n",
    "    \"\"\"\n",
    "    Collate function which to transform scipy csr matrix to pytorch sparse tensor\n",
    "    \"\"\"\n",
    "    # batch[0] since it is returned as a one element list\n",
    "\n",
    "    customer_batch, product_batch, targets_batch = batch[0]\n",
    "    \n",
    "    if type(customer_batch[0]) == csr_matrix:\n",
    "        customer_batch = customer_batch.tocoo() # removed vstack\n",
    "        customer_batch = sparse_to_tensor(customer_batch)\n",
    "    else:\n",
    "        customer_batch = torch.DoubleTensor(customer_batch)\n",
    "\n",
    "    if type(product_batch[0]) == csr_matrix:\n",
    "        product_batch = product_batch.tocoo() # removed vstack\n",
    "        product_batch = sparse_to_tensor(product_batch)\n",
    "    else:\n",
    "        product_batch = torch.DoubleTensor(product_batch)\n",
    "    \n",
    "    if type(targets_batch[0]) == csr_matrix:\n",
    "        targets_batch = targets_batch.tocoo() # removed vstack\n",
    "        targets_batch = sparse_to_tensor(targets_batch)\n",
    "    else:\n",
    "        targets_batch = torch.DoubleTensor(targets_batch)\n",
    "    \n",
    "    return customer_batch, product_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMSaleTrainDataLoader(Dataset):\n",
    "    \"\"\"HMSaleTrainDataLoader Training set of HM sales data\n",
    "\n",
    "    Args:\n",
    "        transactions (pd.DataFrame): Dataframe of transaction records\n",
    "        all_products_id (list): A list contains all product ids\n",
    "    \"\"\"\n",
    "    def __init__(self, transactions, all_products_id):\n",
    "        self.customers, self.products, self.labels = self.get_dataset(transactions, all_products_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.customers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.customers[idx], self.products[idx], self.labels[idx]\n",
    "    \n",
    "    def get_dataset(self, transactions, all_products_id):\n",
    "        customers, products, labels = [], [], []\n",
    "        customer_product_set = set(zip(transactions['customer_id'], transactions['article_id']))\n",
    "        \n",
    "        \"\"\"negative sampling\"\"\"\n",
    "        # set up negative:positive ratio as 4:1\n",
    "        negative_samples = 4\n",
    "\n",
    "        for u, i in tqdm(customer_product_set):\n",
    "            customers.append(u)\n",
    "            products.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(negative_samples):\n",
    "                negative_product = np.random.choice(all_products_id)\n",
    "                while (u, negative_product) in customer_product_set:\n",
    "                    negative_product = np.random.choice(all_products_id)\n",
    "                customers.append(u)\n",
    "                products.append(negative_product)\n",
    "                labels.append(0)\n",
    "        return customers, products, torch.tensor(labels)\n",
    "    \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_label = torch.softmax(y_pred, dim=1)\n",
    "    _, y_pred_label = torch.max(y_pred_label, dim = 1)\n",
    "    correct_pred = (y_pred_label == y_test).sum()\n",
    "    acc = correct_pred/y_test.shape[0]\n",
    "    return acc  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    \"\"\"NCF - Neural Collaborative Filtering proposed by He et al.\n",
    "\n",
    "    Args:\n",
    "        num_users (int): Number of users\n",
    "        num_items (iut): Number of products\n",
    "        transactions (pd.DataFrame): Dataframe of transaction records\n",
    "        all_products_id (list): A list contains all product ids\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_users, \n",
    "            num_items, \n",
    "            embedding_dim: int = 10,\n",
    "            hidden_size: int = 64,\n",
    "            output_size: int = 32,\n",
    "            num_hidden_layers: int = 1,\n",
    "            num_classes: int = 2,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.user_embedding_layer = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "            self.item_embedding_layer = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fcs = nn.Sequential()\n",
    "            if embedding_dim is not None:\n",
    "                in_channels = (\n",
    "                    [embedding_dim + embedding_dim] \n",
    "                    + [hidden_size]*num_hidden_layers \n",
    "                    + [output_size]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError\n",
    "            for i in range(len(in_channels)):\n",
    "                if i != len(in_channels)-1:\n",
    "                    self.fcs.append(nn.Linear(in_features=in_channels[i],  out_features=in_channels[i+1]))\n",
    "                else:\n",
    "                    self.fcs.append(nn.Linear(in_features=in_channels[i],  out_features=num_classes))\n",
    "    \n",
    "    def forward(self, user_input, item_input):\n",
    "\n",
    "        user_embedding = self.user_embedding_layer(user_input)\n",
    "        item_embedding = self.item_embedding_layer(item_input)\n",
    "        concat_embedding = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        \n",
    "        for fc_layer in self.fcs:\n",
    "            concat_embedding = fc_layer(concat_embedding)\n",
    "            concat_embedding = self.relu(concat_embedding)\n",
    "        # print(\"TEST\")\n",
    "        # print(concat_embedding)\n",
    "        #pred = F.softmax(concat_embedding)\n",
    "        return concat_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 2668\n",
      "num_items: 31159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab09e26eb1b45be8f50520994bde697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (user_embedding_layer): Embedding(2668, 10)\n",
      "  (item_embedding_layer): Embedding(31159, 10)\n",
      "  (relu): ReLU()\n",
      "  (fcs): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76685a8a7f93405ea265080dfe2c6616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Loss: 0.503244 | Acc: 0.8000\n",
      "Epoch 001: | Loss: 0.480977 | Acc: 0.8000\n",
      "Epoch 002: | Loss: 0.460374 | Acc: 0.8000\n",
      "Epoch 003: | Loss: 0.445362 | Acc: 0.8000\n",
      "Epoch 004: | Loss: 0.426127 | Acc: 0.8000\n",
      "Epoch 005: | Loss: 0.401630 | Acc: 0.8000\n",
      "Epoch 006: | Loss: 0.373798 | Acc: 0.8000\n",
      "Epoch 007: | Loss: 0.342097 | Acc: 0.8493\n",
      "Epoch 008: | Loss: 0.308760 | Acc: 0.8705\n",
      "Epoch 009: | Loss: 0.277887 | Acc: 0.8861\n",
      "Epoch 010: | Loss: 0.250040 | Acc: 0.9001\n",
      "Epoch 011: | Loss: 0.224752 | Acc: 0.9120\n",
      "Epoch 012: | Loss: 0.201982 | Acc: 0.9218\n",
      "Epoch 013: | Loss: 0.181652 | Acc: 0.9290\n",
      "Epoch 014: | Loss: 0.162129 | Acc: 0.9362\n",
      "Epoch 015: | Loss: 0.143862 | Acc: 0.9432\n",
      "Epoch 016: | Loss: 0.128279 | Acc: 0.9490\n",
      "Epoch 017: | Loss: 0.116868 | Acc: 0.9531\n",
      "Epoch 018: | Loss: 0.106592 | Acc: 0.9568\n",
      "Epoch 019: | Loss: 0.098257 | Acc: 0.9599\n",
      "Epoch 020: | Loss: 0.091570 | Acc: 0.9620\n",
      "Epoch 021: | Loss: 0.085412 | Acc: 0.9643\n",
      "Epoch 022: | Loss: 0.082107 | Acc: 0.9651\n",
      "Epoch 023: | Loss: 0.077983 | Acc: 0.9667\n",
      "Epoch 024: | Loss: 0.075314 | Acc: 0.9675\n",
      "Epoch 025: | Loss: 0.071818 | Acc: 0.9685\n",
      "Epoch 026: | Loss: 0.068069 | Acc: 0.9700\n",
      "Epoch 027: | Loss: 0.066904 | Acc: 0.9703\n",
      "Epoch 028: | Loss: 0.066526 | Acc: 0.9705\n",
      "Epoch 029: | Loss: 0.063800 | Acc: 0.9716\n",
      "Epoch 030: | Loss: 0.063124 | Acc: 0.9715\n",
      "Epoch 031: | Loss: 0.060852 | Acc: 0.9723\n",
      "Epoch 032: | Loss: 0.060027 | Acc: 0.9728\n",
      "Epoch 033: | Loss: 0.058524 | Acc: 0.9734\n",
      "Epoch 034: | Loss: 0.057874 | Acc: 0.9737\n",
      "Epoch 035: | Loss: 0.057329 | Acc: 0.9739\n",
      "Epoch 036: | Loss: 0.056379 | Acc: 0.9741\n",
      "Epoch 037: | Loss: 0.055575 | Acc: 0.9744\n",
      "Epoch 038: | Loss: 0.054795 | Acc: 0.9746\n",
      "Epoch 039: | Loss: 0.054618 | Acc: 0.9747\n",
      "\n",
      "Best Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "# set up hyper-parameters\n",
    "learning_rate = 0.01\n",
    "epoch = 40\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set up dataset for training\n",
    "num_users = len(train_transactions.customer_id.unique())\n",
    "print(\"num_users:\", num_users)\n",
    "num_items = len(all_products_id)\n",
    "print(\"num_items:\", num_items)\n",
    "train_data = HMSaleTrainDataLoader(train_transactions, all_products_id)\n",
    "train_loader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "# initiate model for training\n",
    "model = NCF(num_users=num_users, num_items=num_items)\n",
    "model.to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "best_acc = 0.0\n",
    "for e in tqdm(range(epoch)):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for customer_batch, product_batch, label_batch in train_loader:\n",
    "        customer_batch, product_batch, label_batch = customer_batch.to(device), product_batch.to(device), label_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(customer_batch, product_batch)\n",
    "        loss = loss_fn(y_pred, label_batch)\n",
    "        acc = binary_acc(y_pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        cur_acc = epoch_acc/len(train_loader)\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.6f} | Acc: {epoch_acc/len(train_loader):.4f}')\n",
    "\n",
    "print(f'\\nBest Accuracy: {best_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31735"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_transactions.article_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4976, 0.5024],\n",
       "        [0.5063, 0.4937]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1, 1])\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.tensor([[0.0004, 0.0100],\n",
    "                     [0.1527, 0.1276]])\n",
    "\n",
    "y_pred_softmax = torch.softmax(input, dim = 1)\n",
    "_, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "\n",
    "y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5024, 0.5063])\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred = (y_pred_tags == y).sum()\n",
    "correct_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7857ada9c2c05e5ee75d334d60fb77cca110ad03375c3b02029444557fa6212d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
